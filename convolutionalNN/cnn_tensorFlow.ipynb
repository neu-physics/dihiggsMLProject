{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To-Do\n",
    "#1) transfer files locally from lxplus or maybe figure out venv on lxplus\n",
    "#2) CM of phi for image?\n",
    "#3) bb-center a la photography talk\n",
    "#4) slide 9: remake these for diHiggs events that have the Higgs back-to-back ie no ISR. Are the blobs more pronounced than non-back-to-back diHiggs events?\n",
    "#5) increase pixels to see if better?\n",
    "#6) compare expected yields post-selection from this to other approaches. Much higher signal stats here! But by how much?\n",
    "#7) subtract average a la Anna's idea\n",
    "#8) store images by jet category?\n",
    "# ...\n",
    "# ...\n",
    "#9) script image making --> different pixel sizes, jet categories\n",
    "#10) compare ROC curves for different approaches a la Evan's work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the needed libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py as h5\n",
    "import os \n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, BatchNormalization, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.regularizers import l2, l1\n",
    "from keras.initializers import Constant\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, '/home/btannenw/Desktop/ML/dihiggsMLProject/')\n",
    "from utils.commonFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image data\n",
    "hh_h5  = h5.File('/home/btannenw/Desktop/ML/dihiggsMLProject/convolutionalNN/pp2hh4b_1M_20files_composite_pix15_phiCM_cat/images/pp2hh4b_1M_20files_composite_pix15_phiCM_cat_allImages.h5', 'r')\n",
    "qcd_h5 = h5.File('/home/btannenw/Desktop/ML/dihiggsMLProject/convolutionalNN/ppTo4b_4M_20files_composite_pix15_phiCM_cat/images/ppTo4b_4M_20files_composite_pix15_phiCM_cat_allImages.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make composite dihiggs images\n",
    "#hh_combImages = []\n",
    "##for iEvt in range(0, len(hh_imgs['trackImgs'])):\n",
    "#for iEvt in range(0, 5000):\n",
    "#    if iEvt%500 == 0:\n",
    "#        print('Processed {} dihiggs events'.format(iEvt) )\n",
    "#    hh_combImages.append( np.stack( (hh_imgs['trackImgs'][iEvt], hh_imgs['nHadronImgs'][iEvt], hh_imgs['photonImgs'][iEvt]), axis=-1) )\n",
    "#print(len(hh_combImages))\n",
    "\n",
    "#hh = np.stack( (hh_combImages), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh_h5.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data\n",
    "#nEvents = 49757\n",
    "#hh = hh_h5['compositeImgs'][:nEvents]\n",
    "#qcd = qcd_h5['compositeImgs'][:nEvents]\n",
    "\n",
    "hh = hh_h5['compositeImgs']\n",
    "qcd = qcd_h5['compositeImgs']\n",
    "print(len(hh), len(qcd))\n",
    "#hh = list(hh)\n",
    "#qcd = list(qcd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nEvents_4j = 19000\n",
    "#hh_4j = hh_h5['compositeImgs_4j'][:nEvents_4j]\n",
    "#qcd_4j = qcd_h5['compositeImgs_4j'][:nEvents_4j]\n",
    "\n",
    "hh_4j = hh_h5['compositeImgs_<4j']\n",
    "qcd_4j = qcd_h5['compositeImgs_<4j']\n",
    "hh_4j0b = hh_h5['compositeImgs_>=4j0b']\n",
    "qcd_4j0b = qcd_h5['compositeImgs_>=4j0b']\n",
    "hh_4j1b = hh_h5['compositeImgs_>=4j1b']\n",
    "qcd_4j1b = qcd_h5['compositeImgs_>=4j1b']\n",
    "\n",
    "hh_4j2b = hh_h5['compositeImgs_>=4j2b']\n",
    "qcd_4j2b = qcd_h5['compositeImgs_>=4j2b']\n",
    "hh_4j3b = hh_h5['compositeImgs_>=4j3b']\n",
    "qcd_4j3b = qcd_h5['compositeImgs_>=4j3b']\n",
    "hh_4j4b = hh_h5['compositeImgs_>=4j4b']\n",
    "qcd_4j4b = qcd_h5['compositeImgs_>=4j4b']\n",
    "\n",
    "hh_g4j4b = hh_h5['compositeImgs_>=4j>=4b']\n",
    "qcd_g4j4b = qcd_h5['compositeImgs_>=4j>=4b']\n",
    "\n",
    "#hh_4j\n",
    "#hh  = np.asarray( [ img for img, njets in zip(hh_h5['compositeImgs'], hh_h5['nJets']) if njets >= 4 ] )\n",
    "#qcd  = np.asarray( [ img for img, njets in zip(qcd_h5['compositeImgs'], qcd_h5['nJets']) if njets >= 4 ] )\n",
    "#hh  = np.asarray( [ img for img in hh_h5['compositeImgs'] ] )\n",
    "#qcd  = np.asarray( [ img for img in qcd_h5['compositeImgs'] ] )\n",
    "\n",
    "hh_all = list(hh_4j) + list(hh_4j0b) + list(hh_4j1b) + list(hh_4j2b) + list(hh_4j3b) + list(hh_4j4b) + list(hh_g4j4b)\n",
    "\n",
    "#hh = []\n",
    "#nEvts = len(hh_h5['nJets'])\n",
    "#for iEvt in range(0, nEvts):\n",
    "#    if (100*(iEvt+1)/nEvts)%10 == 0:\n",
    "#        print( 'Processed {}% hh...'.format( (100*(iEvt+1))/nEvts))\n",
    "#    if hh_h5['nJets'][iEvt]>3:\n",
    "#        hh.append(hh_h5['compositeImgs'][iEvt])\n",
    "\n",
    "print(len(hh), len(qcd), len(hh_4j)+len(hh_4j0b)+len(hh_4j1b)+len(hh_4j2b)+len(hh_4j3b)+len(hh_4j4b)+len(hh_g4j4b), len(hh_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nEvents = min(len(hh), len(qcd))\n",
    "#nEvents = min(len(hh_imgs['compositeImgs']), len(qcd_imgs['compositeImgs']))\n",
    "\n",
    "# Make labels\n",
    "hh_labels = np.ones( len(hh) )\n",
    "qcd_labels = np.zeros( len(qcd) )\n",
    "\n",
    "# Make combined dataset\n",
    "#all_images = np.concatenate ( (hh.copy(), qcd.copy()), axis=0)\n",
    "all_images = np.concatenate ( (hh, qcd) )\n",
    "all_labels = np.concatenate ( (hh_labels.copy(), qcd_labels.copy()), axis=0)\n",
    "print(all_images.shape, all_labels.shape)\n",
    "\n",
    "#all_images, all_labels = shuffle(all_images, all_labels, random_state=0)\n",
    "imgs_train, imgs_test, labels_train, labels_test = train_test_split(all_images, all_labels, test_size=0.25, shuffle= True, random_state=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layer options\n",
    "l2_reg = tf.keras.regularizers.l2(1e-4)\n",
    "conv_kwargs = dict(\n",
    "            activation=\"relu\",\n",
    "            #kernel_initializer=tf.keras.initializers.lecun_normal(),\n",
    "            #kernel_regularizer=l2_reg,\n",
    "    )\n",
    "\n",
    "dense_kwargs = conv_kwargs\n",
    "\n",
    "# bias options\n",
    "initial_bias = np.log([len(hh)/len(qcd)])\n",
    "output_bias = Constant(initial_bias)\n",
    "print(\"initial bias: {}\".format(initial_bias))\n",
    "\n",
    "# class weights\n",
    "# Scaling by total/2 helps keep the loss to a similar magnitude.\n",
    "# The sum of the weights of all examples stays the same.\n",
    "total = len(qcd) + len(hh)\n",
    "weight_for_0 = (1 / len(qcd))*(total)/2.0 \n",
    "weight_for_1 = (1 / len(hh))*(total)/2.0\n",
    "class_weight = {0: weight_for_0, 1: weight_for_1}\n",
    "print('Weight for class 0: {:.2f}'.format(weight_for_0))\n",
    "print('Weight for class 1: {:.2f}'.format(weight_for_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateModel( _model, _history, _data, _labels):\n",
    "    # *** A. Make prediction data\n",
    "    hh_data_test    = np.asarray([x for x,y in zip(_data, _labels) if y==1])\n",
    "    hh_labels_test  = np.asarray([y for x,y in zip(_data, _labels) if y==1])\n",
    "    qcd_data_test   = np.asarray([x for x,y in zip(_data, _labels) if y==0])\n",
    "    qcd_labels_test = np.asarray([y for x,y in zip(_data, _labels) if y==0])\n",
    "\n",
    "    # *** B. Load model \n",
    "\n",
    "    # *** C. Make history plots\n",
    "    print(\"*** Training History Plots\")\n",
    "    makeHistoryPlots( _history, ['accuracy', 'loss', 'auc'] )\n",
    "\n",
    "    # *** C. Make predictions\n",
    "    score_hh = _model.evaluate(hh_data_test, hh_labels_test)\n",
    "    score_qcd = _model.evaluate(qcd_data_test, qcd_labels_test)\n",
    "    print(score_hh, score_qcd)\n",
    "    pred_hh = _model.predict(hh_data_test)\n",
    "    pred_qcd = _model.predict(qcd_data_test)\n",
    "    \n",
    "    # *** D. make output score plot\n",
    "    print(\"*** Output Score Plot\")\n",
    "    _nBins = 40\n",
    "    predictionResults = {'hh_pred':pred_hh, 'qcd_pred':pred_qcd}\n",
    "    compareManyHistograms( predictionResults, ['hh_pred', 'qcd_pred'], 2, 'Signal Prediction', 'CNN Score', 0, 1, _nBins, _yMax = 5, _normed=True, savePlot=False )\n",
    "\n",
    "    # *** E. Get best cut value for CNN assuming some minimal amount of signal\n",
    "    pred_hh_sig = [x[0] for x in pred_hh.copy()]\n",
    "    pred_qcd_sig = [x[0] for x in pred_qcd.copy()]\n",
    "\n",
    "    sig, cut, sigErr = returnBestCutValue('CNN', pred_hh_sig, pred_qcd_sig, _minBackground=400e3, _testingFraction=0.025)\n",
    "    \n",
    "    # *** F. Confusion matrix\n",
    "    print(\"*** Confusion Matrix\")\n",
    "    preds_test = _model.predict(_data)\n",
    "    cm = confusion_matrix( _labels, preds_test > cut)\n",
    "    print(cm)\n",
    "    print('QCD called QCD (True Negatives): {} ({}%)'.format( cm[0][0], round(100*(cm[0][0]/sum(cm[0]))) ))\n",
    "    print('QCD called Dihiggs (False Positives):  {} ({}%)'.format( cm[0][1], round(100*(cm[0][1]/sum(cm[0]))) ))\n",
    "    print('Dihiggs called QCD (False Negatives):  {} ({}%)'.format( cm[1][0], round(100*(cm[1][0]/sum(cm[1]))) ))\n",
    "    print('Dihiggs called Dihiggs (True Positives):  {} ({}%)'.format( cm[1][1], round(100*(cm[1][1]/sum(cm[1]))) ))\n",
    "    print('Total Dihiggs: ', np.sum(cm[1]))\n",
    "    \n",
    "    # *** 7. Make ROC curve\n",
    "    print(\"*** ROC Curve\")\n",
    "    makeEfficiencyCurves( dict(label=\"CNN\", labels=_labels, prediction=preds_test, color=\"blue\"), _modelName='CNN')\n",
    "    #utils/commonFunctions.py:def makeEfficiencyCurves(*data, _modelName='', savePlot=False, saveDir=''):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeModelCNN():\n",
    "    # Define model\n",
    "    _model = Sequential()\n",
    "\n",
    "    # Convolutional part\n",
    "    _model.add( Conv2D(16, (3, 3), input_shape=(15, 15, 3), **conv_kwargs) )\n",
    "    _model.add( MaxPooling2D((2, 2)) )\n",
    "    _model.add( Conv2D(32, (3, 3), **conv_kwargs))\n",
    "    _model.add( MaxPooling2D((2, 2)))\n",
    "    _model.add( Conv2D(32, (2, 2), **conv_kwargs))\n",
    "    #model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "\n",
    "    # Feed-forward part\n",
    "    _model.add( Flatten())\n",
    "    _model.add( Dense(64, **dense_kwargs))\n",
    "    _model.add( BatchNormalization() )\n",
    "    _model.add( Dense(64, **dense_kwargs))\n",
    "    #model.add( Dropout(0.2) )\n",
    "    _model.add( Dense(1, activation='sigmoid', bias_initializer=output_bias) )\n",
    "    \n",
    "    _model.summary()\n",
    "    \n",
    "    metrics = [ #tf.keras.metrics.categorical_accuracy,\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.AUC(name='auc'),\n",
    "    ]\n",
    "\n",
    "    _model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=metrics)\n",
    "              #metrics=['accuracy'])\n",
    "    \n",
    "    return _model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** 1. Define output directory\n",
    "topDir = \"composite_pixel25_3conv\"\n",
    "name = 'epochs15'\n",
    "if not os.path.exists(topDir):\n",
    "    os.makedirs(topDir)\n",
    "model_dir = os.path.join(topDir, \"\", \"models\", name)\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "    \n",
    "# *** 2. Define callbacks for training\n",
    "fit_callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=os.path.join(model_dir, name)+'.hdf5',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            #monitor=\"val_auc\",\n",
    "            #mode=\"max\",\n",
    "            monitor=\"val_loss\",\n",
    "            mode=\"min\",\n",
    "        ),\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "            #monitor=\"val_auc\",\n",
    "            #mode=\"max\",\n",
    "            monitor='val_loss', \n",
    "            mode='min', \n",
    "            verbose=1, \n",
    "            patience=15,  \n",
    "            min_delta=.0025,\n",
    "        ),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** 3A. Train without weights\n",
    "\n",
    "modelNoWeights = makeModelCNN()\n",
    "\n",
    "historyNoWeights = modelNoWeights.fit(imgs_train, labels_train, epochs=50, \n",
    "                    shuffle=True,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(imgs_test, labels_test),\n",
    "                    callbacks=fit_callbacks,\n",
    "                    )\n",
    "\n",
    "evaluateModel( modelNoWeights, historyNoWeights, imgs_test, labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelWeighted = makeModelCNN()\n",
    "\n",
    "historyWeighted = modelWeighted.fit(imgs_train, labels_train, epochs=50, \n",
    "                    shuffle=True,\n",
    "                    batch_size=512,\n",
    "                    validation_data=(imgs_test, labels_test),\n",
    "                    callbacks=fit_callbacks,\n",
    "                    class_weight=class_weight\n",
    "                   )\n",
    "\n",
    "evaluateModel( modelWeighted, historyWeighted, imgs_test, labels_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.0, 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='loss')\n",
    "plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.ylim([0., 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['auc'], label='AUC')\n",
    "plt.plot(history.history['val_auc'], label = 'val_AUC')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.ylim([0., 1])\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "#test_loss, test_acc = model.evaluate(imgs_test,  labels_test, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** 4B. Get signifiance for any user-specified NN score cut value\n",
    "testingFraction = 0.1\n",
    "lumiscale_hh  = getLumiScaleFactor(testingFraction, True, 25e3)\n",
    "lumiscale_qcd = getLumiScaleFactor(testingFraction, False, 50e3)\n",
    "cut = 0.48\n",
    "_nSignal = sum( value > cut for value in pred_hh_sig)*lumiscale_hh\n",
    "_nBackground = sum( value > cut for value in pred_qcd_sig)*lumiscale_qcd\n",
    "\n",
    "print('nSig = {0} , nBkg = {1} with significance = {2} for NN score > {3}'.format(_nSignal, _nBackground, _nSignal/np.sqrt(_nBackground), cut) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
