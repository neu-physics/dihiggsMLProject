\label{sec:RandomForest}
Random Forest algorithms share a similar tree structure with BDTs, but they leverage ensembles of independent decision trees as opposed to iteratively improving the predictions of a single tree using misclassified events. Each tree in a random forest is `grown` using a random sampling of input variables and training events. The randomness of the sampling ensures each tree yields a unique but correlated prediction compared to the other trees in the forest. The class prediction of the forest is the majority vote of the constituent trees. Tuning the hyperparameters of a random forest requires optimizing the number of trees in the forest, the variable sub-sampling used to produce each tree, and the depth of the constituent trees.

The random forest trained for dihiggs classification uses the reconstruction algorithm that selects dijet pairs consistent with a Higgs mass hypothesis, and the top seventeen reconstructed and event-level variables were used as input. The random forest was implemented using the 'XGBRFClassifier` functionality from xgboost~\cite{xgboost}. An optimal forest was obtained by individually varying each hyperparameter over a reasonable range and selecting the best performing model. The optimal set of hyperparameters consisted of training with 300 constituent trees, a maximum tree depth of 20, column sub-sampling rate of 0.8, and an L1 regularzation term of 1.175. The best significance for the random forest approach was found to be $S/\sqrt{B}$ = 2.04$\pm$0.07 when requiring a prediction score > 0.70. Prediction results are shown in Figure~\ref{fig:rf_score}.

\begin{figure}[!h] 
\begin{center}
\includegraphics*[width=0.75\textwidth] {randomForest/figures/rf_score_v2}
\caption{Output score on the testing dataset with the fully trained random forest classifier.}
  \label{fig:rf_score}
\end{center}
\end{figure}


