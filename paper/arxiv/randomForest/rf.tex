\label{sec:RandomForest}
Random Forest algorithms share a similar tree structure with BDTs, but they leverage ensembles of independent decision trees as opposed to iteratively improving the predictions of a single tree using misclassified events. Each tree in a random forest is `grown` using a random sampling of input variables and training events. The randomness of the sampling ensures each tree yields a unique but correlated prediction compared to the other trees in the forest. The class prediction of the forest is the majority vote of the constituent trees. Tuning the hyperparameters of a random forest requires optimizing the number of trees in the forest, the variable sub-sampling used to produce each tree, and the depth of the constituent trees.

The random forest trained for dihiggs classification uses the reconstruction algorithm that selects dijet pairs consistent with a Higgs mass hypothesis, and the top seventeen reconstructed and event-level variables were used as input. The input variables were selected as having the highest separability between dihiggs and QCD processes. The random forest was implemented using the 'XGBRFClassifier` functionality from xgboost~\cite{xgboost}. Detail the hyperparameters. Predictions were made on data fully independent from the training data, and the results are shown in Figure~\ref{fig:rf_score}. The best significance for the random forest approach is found to be $S/\sqrt{B}$ = 2.04$\pm$0.07 when requiring a prediction score > 0.70. 

\begin{figure}[!h] 
\begin{center}
\includegraphics*[width=0.75\textwidth] {randomForest/figures/rf_score_v2}
\caption{Output score on the testing dataset with the fully trained random forest classifier.}
  \label{fig:rf_score}
\end{center}
\end{figure}


