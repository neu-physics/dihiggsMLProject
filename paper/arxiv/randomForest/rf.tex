\label{sec:RandomForest}
Random Forest algorithms share a similar tree structure with BDTs, but they leverage ensembles of independent decision trees as opposed to iteratively improving the predictions of a single tree using misclassified events. Each tree in a random forest is `grown' using a random sampling of input variables and training events. The randomness of the sampling ensures each tree yields a unique but correlated prediction compared to the other trees in the forest. The class prediction of the forest is the majority vote of the constituent trees. Tuning the hyperparameters of a random forest requires optimizing the number of trees in the forest, the variable sub-sampling used to produce each tree, and the depth of the constituent trees.

The random forest trained for di-Higgs classification uses the reconstruction algorithm that selects di-jet pairs consistent with a Higgs mass hypothesis, and the same reconstructed and event-level variables used for the BDT were used as input to the forest. The random forest was implemented using xgboost~\cite{xgboost}. Five hyperparameters were found to have a meaningful impact on the performance of the model: the number of trees in the forest, the maximum tree depth, a L1 regularization term, the fraction of events used as inputs to each tree (sub-sample fraction), and the fraction of reconstruction variables randomly selected as inputs for each tree (the column sub-sampling rate). Similar to the approach for optimizing the hyperparameters of the BDT, an optimal random forest configuration was obtained by individually varying each significant hyperparameter over a reasonable range and selecting the best performing model.

The optimal random forest was training with 300 constituent trees, a maximum tree depth of 20, a L1 regularization term of 1.175, a sub-sampling fraction of 0.8, and a column sub-sampling rate of 0.8,. The best significance for the random forest approach was found to be $\sigma$ = 2.44$\pm$0.19 when requiring a prediction score > 0.80. This region yielded an expectation of $544.7 \pm 6.3$ signal events and $5.0 \pm 0.5$ $\cdot$ $10^4$ background events. Distributions of the predictions for signal and background are shown in Figure~\ref{fig:rf_score}.

\begin{figure}[!h] 
\begin{center}
\includegraphics*[width=0.75\textwidth] {randomForest/figures/rf_score_v3}
\caption{Output score on the testing dataset with the fully trained random forest classifier.}
  \label{fig:rf_score}
\end{center}
\end{figure}


