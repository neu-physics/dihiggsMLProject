\label{sec:NN}
Fully connected or feed-forward neural networks (ffNN or NN) have a long history in high energy physics. One of the earliest applications of this type of approach was in a search for top quark production using the CDF experiment at the Tevatron. The fundamental element of a feed-forward neural network is called a `layer`, and these layers connect input variables to a predicted outcome which is evaluated against a known target value. A fully-connected network can have multiple internal (or hidden) layers between the input and output layers, and each hidden layer is composed of a series of trainable activation functions and weights that allow the network to identify and iteratively combine important features of the input space. A vector of relevant physics-level information (e.g. mass of two highest \pt jets, angle between measured objects, etc) is constructed for each event, and these vectors are then `fed forward` through multiple layers in order to predict whether the event comes from a signal dihiggs process or a background QCD process. A function is chosen to quantify the difference between the model prediction and target values, and the loss is used to adjust the network weights in the next training iteration in a process called backpropagation. The training stops and the model is fully trained once the improvement in the loss between iterations falls beneath some user-defined threshold.


Using the KS test method, the 22 best variables for the ‘equalDijetMass’ reconstruction algorithm were chosen to be the input variables for the feed forward neural network (FFNN).

The NN used for dihiggs classification is built using the Keras~\cite{chollet2015keras} and Tensorflow~\cite{tensorflow} pacakges. Input events are reconstructed using the `closestDijetToHiggsMass’ reconstruction algorithm, and the top twenty-two variables identified as most-separable are used as the input vector. The complete NN structure consists the an input layer, two hidden layers, and an single-node output layer. The first hidden layer contains 175 nodes with an L2 kernel regularizer (lambda = $10^{-4}$. The second hidden layer contains 90 nodes with no kernel regularizer. A batch normalization layer and a dropout (0.2) function are placed in between the two hidden layers to prevent over-fitting. Both hidden layers use a rectified linear (ReLU) activation function, while the output layer uses a sigmoid activation function. A schematic flowchart of the network structure is shown in Figure~\ref{fig:nn}
%The sequential model was compiled using the ‘adam’ optimizer along with the ‘binary\_crossentropy’ loss method. Finally, the model was fit on the training data along with the validation data for 100 epochs. 

\begin{figure}[!h] 
\begin{center}
\includegraphics*[width=0.75\textwidth] {ffNN/figures/flowchart_ffNN.png}
\caption{Structure of the feed-forward neural network. The input variables are fed through two fully connected dense layers to classify events. One dropout layer and one batch normalization layer help mitigate over-fitting during training.}
  \label{fig:nn}
\end{center}
\end{figure}

The hyperparameters of the feed-forward NN ($N_{nodes}$ in each hidden layer, learning rate, regularization, etc) were optimized by something more rigorous than trial and error. Describe the process semi in-depth and talk about what effect different parameters had on the overall performance. The model was trained for 25 epochs before the minimal loss-improvement threshold was met, and the results are shown in Figure~\ref{fig:results_nn}. The trained model obtained a maximum $S/\sqrt{B}$ = 1.9$\pm$0.09 when considering events with a signal prediction score > 0.89. This phase-space has a signal yield of 1303.73 events and a background yield of 467273.3 events.

\begin{figure}[!h] 
\begin{center}
  \subfloat[]{\includegraphics[width = 3in]{ffNN/figures/acc_ffnn}} 
  \subfloat[]{\includegraphics[width = 3in]{ffNN/figures/score_ffnn}}\\
\caption{Accuracy (left) for the testing and training datasets and the final predictions (right) of the feed-forward network.}
  \label{fig:results_nn}
\end{center}
\end{figure}

