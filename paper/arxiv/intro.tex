\section{Introduction}
\label{sec:intro}

Talk about machine learning (ML). It's really cool. Also most of it is just ideas that people came up with in the 90s and we only now have sufficient computing architecture. Yes Chris used a simple neural network in his thesis at the Tevatron, but honestly, who cares about history these days?

Deep networks have a lot of really cool potential and are pretty easy to implement. The usual feed-forward approach isn't too complicated, but it can have lots of hyperparameters to worry about! You can also do lots of cool stuff that involve emergent information like Lorentz Boost Networks or maybe even energyflow stuff. These approaches raise interesting questions about what sorts of information are best to feed to our networks; things that are best for physicists to learn from might not be optimal for machines!

You can even think of fun semi-supervised approaches like clustering and maybe some stuff with auto-encoders. To be totally honest, I'm not really sure if the auto-encoder stuff is relevant for this paper, but it's really interesting and is maybe cool for unexpected BSM signatures? Could be a cool side-study.

All of this is really interesting, but it's important to ground it in reality. High energy physicists care about stuff like dihiggs production because it's lit. The problem is that it's really rare. And the most common branching fraction ($hh\rightarrow b\bar{b}$) is swamped by orders more magnitude similar-appearing events from QCD. The goal of this paper is to show how effective different machine learning techniques are at overcoming this issue and leading to potential Higgs self-coupling measurements at the LHC.

Section Two deals with the physics relevant for dihiggs production and the QCD background. Sections 3 and 4 summarize the various ML methods we played with, and Section 5 compares the results from the various approaches.
