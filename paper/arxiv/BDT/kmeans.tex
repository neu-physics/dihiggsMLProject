\label{sec:kmeans}
K-means clustering is an unsupervised learning algorithm that finds natural unlabeled groupings in a scaled phase-space describing the input dataset. Scaled data is created by compressing or stretching each variable of the dataset to the same maximum and minimum values. This ensures each variable is treated equally when calculated the distances between neighboring points. Minimum values of 0 and maximum values of 1 were chosen for all variables in this analysis.

The k-means approach creates clusters by randomly seeding a user-specified number of cluster centroids in the scaled phase-space. Each point is associated to the closest nearby centroid, and the group of points belonging to a single centrioid is defined as a cluster. The centroid positions are updated over several iterations by minimizing the ensemble sum of the squared distance between a centroid and all points associated to that cluster. As the centroids move, points can be associated to different clusters eventually obtaining a set of clusters defined by locally dense groupings of similar points. Combining unsupervised clustering with the supervised structure of a BDT converts the unsupervised approach into a semi-supervised algorithm whose performance can be compared to other supervised methods.

The number of clusters, $k$, to fit is a user-defined hyperparameter, and the number of clusters used to describe a dataset must be carefully chosen. Using too few clusters risks not being able to identify meaningfully different populations in the input dataset. Using too many clusters risks losing predictive power by splitting coherent sets of points into arbitrary groupings. An optimal $k$ is found by scanning several values, calculating the total distance between all points and their associated centroid, and selecting a $k$ that minimizes this distance without asymptotically approaching zero.

The optimal $k$ for the di-Higgs data was found lie around 20, and three different clusterings ($k$= 15, 20, 40) were tested for completeness. The scenarios with 15 and 40 clusters test the effects of under-clustering and over-clustering respectively. Two sets of clustered data were used as input to the nominal BDT to test the performance of this semi-supervised approach. The first set used all reconstructed kinematic variables as the input to the $k$-means clustering stage before training the BDT. The second set was produced by performing a principal component analysis (PCA) decomposition on the nominal kinematic inputs before passing through the clustering step and the BDT. PCA is a technique for finding an orthogonal basis of the input data that minimizes the variance along each new axis. No transformation was found to improve the performance of the nominal configuration, and the results are shown in Table~\ref{tab:bdtPCACluster}.

\begin{table}[h!]
\label{tab:bdtPCACluster}
\begin{center}
  %\hskip-4.0cm
    \begin{tabular}{|l|c|c|c|} % <-- Alignments: 1st column left, 2nd middle and 3rd right, with vertical lines in between
      \hline\hline
      \textbf{Method} & $\sigma$ & N$_{\textrm{sig}}$ & N$_{\textrm{bkg}}$ \\
      \hline
      Nominal BDT & 1.84 $\pm$ 0.09 & 986.3  & 2.9$\cdot 10^5$ \\
      15 Clusters & 1.29 $\pm$ 0.02 & 2100.2 & 2.7$\cdot 10^6$ \\
      15 Clusters + PCA & 1.25 $\pm$ 0.02 & 2189.5 & 3.1$\cdot 10^6$ \\         
      20 Clusters & 1.30 $\pm$ 0.02 & 2260.6 & 3.0$\cdot 10^6$ \\
      20 Clusters + PCA & 1.27 $\pm$ 0.03 & 21756.4 & 1.9$\cdot 10^6$ \\         
      40 Clusters & 1.44 $\pm$ 0.03 & 1704.6 & 1.4$\cdot 10^6$ \\
      40 Clusters + PCA & 1.34 $\pm$ 0.02 & 2144.5 & 2.0$\cdot 10^6$ \\         
      \hline\hline
    \end{tabular}
    \caption{Significance and yields showing BDT performance when using the nominal kinematic inputs, clustered kinematic inputs, and clustered inputs from a PCA decomposition. All yields are normalized to full HL-LHC dataset of 3000 fb$^{-1}$.}
    \end{center}
\end{table}
