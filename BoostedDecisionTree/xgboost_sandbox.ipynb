{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78876 rows of dihiggs data Index(['hh_mass', 'h1_mass', 'h2_mass', 'hh_pt', 'h1_pt', 'h2_pt',\n",
      "       'deltaR(h1, h2)', 'deltaR(h1 jets)', 'deltaR(h2 jets)',\n",
      "       'deltaPhi(h1, h2)', 'deltaPhi(h1 jets)', 'deltaPhi(h2 jets)', 'met',\n",
      "       'met_phi', 'scalarHT', 'nJets', 'nBTags', 'jet1_pt', 'jet2_pt',\n",
      "       'jet3_pt', 'jet4_pt', 'jet1_eta', 'jet2_eta', 'jet3_eta', 'jet4_eta',\n",
      "       'jet1_phi', 'jet2_phi', 'jet3_phi', 'jet4_phi', 'jet1_mass',\n",
      "       'jet2_mass', 'jet3_mass', 'jet4_mass', 'jet1_px', 'jet2_px', 'jet3_px',\n",
      "       'jet4_px', 'jet1_py', 'jet2_py', 'jet3_py', 'jet4_py', 'jet1_pz',\n",
      "       'jet2_pz', 'jet3_pz', 'jet4_pz', 'jet1_energy', 'jet2_energy',\n",
      "       'jet3_energy', 'jet4_energy', 'jet1_btag', 'jet2_btag', 'jet3_btag',\n",
      "       'jet4_btag', 'isSignal'],\n",
      "      dtype='object')\n",
      "29176 rows of qcd data Index(['hh_mass', 'h1_mass', 'h2_mass', 'hh_pt', 'h1_pt', 'h2_pt',\n",
      "       'deltaR(h1, h2)', 'deltaR(h1 jets)', 'deltaR(h2 jets)',\n",
      "       'deltaPhi(h1, h2)', 'deltaPhi(h1 jets)', 'deltaPhi(h2 jets)', 'met',\n",
      "       'met_phi', 'scalarHT', 'nJets', 'nBTags', 'jet1_pt', 'jet2_pt',\n",
      "       'jet3_pt', 'jet4_pt', 'jet1_eta', 'jet2_eta', 'jet3_eta', 'jet4_eta',\n",
      "       'jet1_phi', 'jet2_phi', 'jet3_phi', 'jet4_phi', 'jet1_mass',\n",
      "       'jet2_mass', 'jet3_mass', 'jet4_mass', 'jet1_px', 'jet2_px', 'jet3_px',\n",
      "       'jet4_px', 'jet1_py', 'jet2_py', 'jet3_py', 'jet4_py', 'jet1_pz',\n",
      "       'jet2_pz', 'jet3_pz', 'jet4_pz', 'jet1_energy', 'jet2_energy',\n",
      "       'jet3_energy', 'jet4_energy', 'jet1_btag', 'jet2_btag', 'jet3_btag',\n",
      "       'jet4_btag', 'isSignal'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7fc5564c9590>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# *** 0. Import Dataset\n",
    "qcd_raw = pd.read_csv('../samples_500k/qcd_outputDataForLearning.csv')\n",
    "hh_raw = pd.read_csv('../samples_500k/dihiggs_outputDataForLearning.csv')\n",
    "\n",
    "hh_raw.head()\n",
    "hh_raw['isSignal'] = 1\n",
    "hh_raw = hh_raw.drop('isMatchable', 1)\n",
    "print(len(hh_raw), \"rows of dihiggs data\", hh_raw.columns)\n",
    "#print( print(hh_raw.loc[[10]]))\n",
    "\n",
    "qcd_raw.head()\n",
    "qcd_raw['isSignal'] = 0\n",
    "qcd_raw = qcd_raw.drop('isMatchable', 1)\n",
    "print(len(qcd_raw), \"rows of qcd data\", qcd_raw.columns)\n",
    "\n",
    "variableNames = ['hh_mass', 'h1_mass', 'h2_mass']\n",
    "#variableNames = ['deltaR(h1, h2)', 'deltaR(h1 jets)', 'deltaR(h2 jets)']\n",
    "#variableNames = ['hh_mass', 'h1_mass', 'h2_mass', 'deltaR(h1, h2)', 'deltaR(h1 jets)', 'deltaR(h2 jets)']\n",
    "\n",
    "hh_raw.hist(column=variableNames[2], bins=100)\n",
    "qcd_raw.hist(column=variableNames[2], bins=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeEqualSamplesWithUserVariables(signal_raw, bkg_raw, userVariables, nEventsForXGB):\n",
    "    \"\"\"function to return 4 dataframes containing user-specified variables and number of events: 1 signal for training, 1 bkg for training, 1 signal for plotting, 1 bkg for plotting\"\"\"\n",
    "    \n",
    "    # *** 0. Reduce dataframes to only desired variables\n",
    "    signal_reduced  = signal_raw[userVariables]\n",
    "    bkg_reduced     = bkg_raw[userVariables]\n",
    "    signal_labels   = signal_raw[ ['isSignal'] ]\n",
    "    bkg_labels      = bkg_raw[ ['isSignal'] ]\n",
    "\n",
    "    # *** 1. Take first nEventsForXGB events for passing to XGB \n",
    "    signal_reducedForXGB  = signal_reduced[:nEventsForXGB]\n",
    "    bkg_reducedForXGB     = bkg_reduced[:nEventsForXGB]\n",
    "    signal_labelsForXGB   = signal_labels[:nEventsForXGB]\n",
    "    bkg_labelsForXGB      = bkg_labels[:nEventsForXGB]\n",
    "\n",
    "    # *** 2. Combine bkg+signal for passing to XGB \n",
    "    all_reducedForXGB  = signal_reducedForXGB.append(bkg_reducedForXGB)\n",
    "    all_labelsForXGB   = signal_labelsForXGB.append(bkg_labelsForXGB)\n",
    "\n",
    "    \n",
    "    # ** 3. Use additional events for unambiguous testing \n",
    "    signal_reducedForPlots  = signal_reduced[nEventsForXGB:len(bkg_reduced)]\n",
    "    bkg_reducedForPlots     = bkg_reduced[nEventsForXGB:len(bkg_reduced)]\n",
    "    signal_labelsForPlots   = signal_labels[nEventsForXGB:len(bkg_reduced)]\n",
    "    bkg_labelsForPlots      = bkg_labels[nEventsForXGB:len(bkg_reduced)]\n",
    "\n",
    "    # *** 4. Sanity check\n",
    "    print(len(all_reducedForXGB), 'rows of data with ', len(all_labelsForXGB), 'labels [XGB]')\n",
    "    print(len(signal_reducedForPlots), 'rows of signal data with ', len(bkg_labelsForPlots), 'rows of background [Plots]')\n",
    "\n",
    "    \n",
    "    return all_reducedForXGB, all_labelsForXGB, signal_reducedForPlots, signal_labelsForPlots, bkg_reducedForPlots, bkg_labelsForPlots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setupAndTrainBDT(all_data, all_labels, _testSize=0.3, _saveModel=False, _maxDepth=3, _eta=0.3, _modelName='reduced_bdt'):\n",
    "    \"\"\"setup and train BDT\"\"\"\n",
    "    \n",
    "    # *** 1. Split dataset into test + train, make correct input objects\n",
    "    data_train, data_test, label_train, label_test = train_test_split(all_data, all_labels, test_size=_testSize)\n",
    "\n",
    "    D_train = xgb.DMatrix(data_train, label=label_train)\n",
    "    D_test = xgb.DMatrix(data_test, label=label_test)\n",
    "    \n",
    "    # *** 2. Define XGB model \n",
    "    param = {\n",
    "        'eta': _eta, \n",
    "        'max_depth': _maxDepth,  \n",
    "        'objective': 'multi:softprob',  \n",
    "        #'objective': 'reg:logistic',  \n",
    "        #'objective': 'binary:logistic',\n",
    "        'num_class': 2} \n",
    "\n",
    "    steps = 5  # The number of training iterations\n",
    "\n",
    "    # *** 3. Train model \n",
    "    model = xgb.train(param, D_train, steps)\n",
    "    \n",
    "    # *** 4. Evaluate model\n",
    "    preds = model.predict(D_test)\n",
    "    best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "    print(\"Precision = {}\".format(precision_score(label_test, best_preds, average='macro')))\n",
    "    print(\"Recall = {}\".format(recall_score(label_test, best_preds, average='macro')))\n",
    "    print(\"Accuracy = {}\".format(accuracy_score(label_test, best_preds)))\n",
    "    \n",
    "    # *** 5. Dump .txt of trees and leaves with decisions if desired\n",
    "    if (_saveModel):\n",
    "        model.dump_model('tree.{0}.txt'.format(_modelName))\n",
    "        pickle.dump(model, open('model.{0}.pkl'.format(_modelName), 'wb'))\n",
    "        # Make a visual of decision tree\n",
    "        #xgb.plot_tree(model,num_trees=1)\n",
    "        #plt.rcParams['figure.figsize'] = [2000, 400]\n",
    "        #plt.show()\n",
    "        #plt.savefig('tree0.png')\n",
    "    \n",
    "    \n",
    "    return model\n",
    "\n",
    "def setupGridAndTrainBDT(_modelName, all_data, all_labels, _testSize=0.3, _saveModel=False):\n",
    "    \"\"\"setup and train using grid of hyperparameters BDT\"\"\"\n",
    "    \n",
    "    # *** 1. Split dataset into test + train, make correct input objects\n",
    "    data_train, data_test, label_train, label_test = train_test_split(all_data, all_labels, test_size=_testSize)\n",
    "\n",
    "    D_train = xgb.DMatrix(data_train, label=label_train)\n",
    "    D_test = xgb.DMatrix(data_test, label=label_test)\n",
    "    \n",
    "    # *** 2. Define XGB hyper-parameter grid \n",
    "    parameters = {\n",
    "        \"eta\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n",
    "        \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n",
    "        #\"min_child_weight\" : [ 1, 3, 5, 7 ],\n",
    "        #\"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n",
    "        #\"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n",
    "        \"objective\": [\"multi:softprob\"],\n",
    "        \"num_class\": [2]\n",
    "        }\n",
    "\n",
    "    steps = 5  # The number of training iterations\n",
    "\n",
    "    # *** 3. Perform grid optimization \n",
    "    clf = xgb.XGBClassifier()\n",
    "    grid = GridSearchCV(clf,\n",
    "                        parameters, n_jobs=4,\n",
    "                        #scoring=\"neg_log_loss\",\n",
    "                        scoring=\"f1\",\n",
    "                        cv=3,\n",
    "                        verbose = 3)\n",
    "\n",
    "    grid_result = grid.fit(data_train, label_train)\n",
    "    best_params = grid_result.best_params_\n",
    "    print(best_params, type(best_params))\n",
    "    \n",
    "    # *** 4. Train model \n",
    "    model = xgb.train( best_params, D_train, steps)\n",
    "    \n",
    "    \n",
    "    # *** 5. Evaluate model\n",
    "    preds = model.predict(D_test)\n",
    "    best_preds = np.asarray([np.argmax(line) for line in preds])\n",
    "\n",
    "    print(\"Precision = {}\".format(precision_score(label_test, best_preds, average='macro')))\n",
    "    print(\"Recall = {}\".format(recall_score(label_test, best_preds, average='macro')))\n",
    "    print(\"Accuracy = {}\".format(accuracy_score(label_test, best_preds)))\n",
    "    \n",
    "    # *** 5. Dump .txt of trees and leaves with decisions if desired\n",
    "    if (_saveModel):\n",
    "        model.dump_model('gridTree.{0}.txt'.format(_modelName))\n",
    "        pickle.dump(model, open('grid-model.{0}.pkl'.format(_modelName), 'wb'))\n",
    "\n",
    "        # Make a visual of decision tree\n",
    "        #xgb.plot_tree(model,num_trees=1)\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBDTOutputAndTree(_model, _modelName, _signalData, _signalLabels, _bkgData, _bkgLabels, _savePlots=False):\n",
    "    \"\"\"make plots of BDT outputs for signal+background probabilities and tree plot\"\"\"\n",
    "    \n",
    "    # *** 1. Make some 1D histograms of signal decision outputs\n",
    "    signal_DMatrix = xgb.DMatrix(_signalData, label=_signalLabels)\n",
    "    preds_signal = _model.predict(signal_DMatrix)\n",
    "    sig_pred_isBkg     = [x[0] for x in preds_signal]\n",
    "    sig_pred_isSignal  = [x[1] for x in preds_signal]\n",
    "    #plt.hist(preds_hh)\n",
    "\n",
    "    # *** 2. Make some 1D histograms of background decision outputs\n",
    "    bkg_DMatrix = xgb.DMatrix(_bkgData, label=_bkgLabels)\n",
    "    preds_bkg = _model.predict(bkg_DMatrix)\n",
    "    bkg_pred_isBkg    = [x[0] for x in preds_bkg]\n",
    "    bkg_pred_isSignal = [x[1] for x in preds_bkg]\n",
    "    #plt.hist(preds_qcd)\n",
    "\n",
    "    # *** 3. Plot feature importance\n",
    "    if(_savePlots):\n",
    "        xgb.plot_importance(_model)\n",
    "        _fig = plt.gcf()\n",
    "        _scope    = _modelName.split(' ')[0].lower()\n",
    "        _variable = 'featureImportance'\n",
    "        _filename  = _scope + '_' + _variable\n",
    "        _fig.savefig( _filename+'.png', bbox_inches='tight' )\n",
    "    \n",
    "    # *** 4. Make dict for plotting with borrowed functions\n",
    "    _nBins = 40\n",
    "    predictionResults = {'hh_pred_isSignal':sig_pred_isSignal, 'hh_pred_isBkg':sig_pred_isBkg, 'qcd_pred_isSignal':bkg_pred_isSignal, 'qcd_pred_isBkg':bkg_pred_isBkg,}\n",
    "    compareManyHistograms( predictionResults, ['hh_pred_isSignal', 'qcd_pred_isSignal'], 2, 'Signal Prediction', 'BDT Score ({0})'.format(_modelName), 0, 1, _nBins, _savePlot=_savePlots )\n",
    "    compareManyHistograms( predictionResults, ['hh_pred_isBkg', 'qcd_pred_isBkg'], 2, 'Bkg Prediction', 'BDT Score ({0})'.format(_modelName), 0, 1, _nBins, _savePlot=_savePlots )\n",
    "\n",
    "    # *** 5. Make plot of 0th tree\n",
    "    #xgb.plot_tree(_model,num_trees=0)\n",
    "    #plt.gcf().set_size_inches(100, 67)\n",
    "    #_fig = plt.gcf()\n",
    "    #plt.show()\n",
    "    \n",
    "    #if(_savePlots):\n",
    "    #    _variable = 'firstTrainedDecisionTree'\n",
    "    #    _filename  = _scope + '_' + _variable\n",
    "    #    _fig.savefig( _filename+'.png' )\n",
    "        \n",
    "    # *** 6. restore figure defaults\n",
    "    plt.rcParams['figure.figsize'] = [6.4, 4.8]\n",
    "\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "def compareManyHistograms( _dict, _labels, _nPlot, _title, _xtitle, _xMin, _xMax, _nBins, _normed=False, _savePlot=False):\n",
    "       \n",
    "    if len(_dict.keys()) < len(_labels):\n",
    "        print (\"!!! Unequal number of arrays and labels. Learn to count better.\")\n",
    "        return 0\n",
    "    \n",
    "    plt.figure(_nPlot)\n",
    "    if _normed:\n",
    "        plt.title(_title + ' (Normalized)')\n",
    "    else:\n",
    "        plt.title(_title)\n",
    "    plt.xlabel(_xtitle)\n",
    "    plt.ylabel('N_events')\n",
    "    _bins = np.linspace(_xMin, _xMax, _nBins)\n",
    "   \n",
    "    y_max = 0\n",
    "    for iLabel in _labels:\n",
    "        plt.hist(_dict[iLabel], _bins, alpha=0.5, density=_normed, label= iLabel+' Events')\n",
    "        \n",
    "        # get values of histgoram to find greatest y\n",
    "        #_y, _x, _ = plt.hist(_dict[iLabel])\n",
    "        #if (_y.max() > y_max):\n",
    "        #    y_max = _y.max()\n",
    "    \n",
    "    # set max y-value of histogram so there's room for legend\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0,14000])\n",
    "    #plt.ylim([0,1.2*y_max])\n",
    "    \n",
    "    #draw legend\n",
    "    plt.legend(loc='upper left')\n",
    "    #plt.text(.1, .1, s1)\n",
    "    \n",
    "    # store figure copy for later saving\n",
    "    fig = plt.gcf()\n",
    "    \n",
    "    # draw interactively\n",
    "    plt.show()\n",
    "    \n",
    "    #save an image file\n",
    "    if(_savePlot):\n",
    "        _scope    = _title.split(' ')[0].lower()\n",
    "        _variable = _xtitle.lstrip('Jet Pair').replace(' ','').replace('[GeV]','').replace('(','_').replace(')','')\n",
    "        _filename  = _scope + '_' + _variable\n",
    "        if _normed:\n",
    "            _filename = _filename + '_norm'\n",
    "        fig.savefig( _filename+'.png', bbox_inches='tight' )\n",
    "    \n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000 rows of data with  20000 labels [XGB]\n",
      "19176 rows of signal data with  19176 rows of background [Plots]\n",
      "Precision = 0.7285426085339366\n",
      "Recall = 0.7261505656814582\n",
      "Accuracy = 0.7255\n",
      "20000 rows of data with  20000 labels [XGB]\n",
      "19176 rows of signal data with  19176 rows of background [Plots]\n",
      "Precision = 0.6173167187142687\n",
      "Recall = 0.6138751195446832\n",
      "Accuracy = 0.6121666666666666\n",
      "20000 rows of data with  20000 labels [XGB]\n",
      "19176 rows of signal data with  19176 rows of background [Plots]\n",
      "Precision = 0.6201751992103545\n",
      "Recall = 0.6180427256352032\n",
      "Accuracy = 0.619\n",
      "20000 rows of data with  20000 labels [XGB]\n",
      "19176 rows of signal data with  19176 rows of background [Plots]\n",
      "Precision = 0.7634093437152392\n",
      "Recall = 0.7631927293654532\n",
      "Accuracy = 0.7628333333333334\n",
      "20000 rows of data with  20000 labels [XGB]\n",
      "19176 rows of signal data with  19176 rows of background [Plots]\n",
      "Precision = 0.7019549848506709\n",
      "Recall = 0.7015683897115982\n",
      "Accuracy = 0.7016666666666667\n"
     ]
    }
   ],
   "source": [
    "nEventsToTrain = 10000\n",
    "\n",
    "# *** 1. Make mix of dihiggs and QCD for h1, h2, hh mass variables\n",
    "variables_massesOnly = ['hh_mass', 'h1_mass', 'h2_mass']\n",
    "massesOnly_data, massesOnly_labels, hh_massesOnly_data, hh_massesOnly_labels, qcd_massesOnly_data, qcd_massesOnly_labels = makeEqualSamplesWithUserVariables(hh_raw, qcd_raw, variables_massesOnly, nEventsToTrain) \n",
    "model_massesOnly = setupAndTrainBDT(massesOnly_data, massesOnly_labels)\n",
    "\n",
    "# *** 2. Make mix of dihiggs and QCD for only anglular-basis jet info (pt, eta, phi, mass) \n",
    "jetLabels = ['1','2','3','4']\n",
    "jetVariables = ['pt', 'eta', 'phi', 'mass']\n",
    "variables_angularJetInfo = ['jet{0}_{1}'.format(iJetLabel, iJetVariable) for iJetLabel in jetLabels for iJetVariable in jetVariables]\n",
    "angularJetInfo_data, angularJetInfo_labels, hh_angularJetInfo_data, hh_angularJetInfo_labels, qcd_angularJetInfo_data, qcd_angularJetInfo_labels = makeEqualSamplesWithUserVariables(hh_raw, qcd_raw, variables_angularJetInfo, nEventsToTrain) \n",
    "model_angularJetInfo = setupAndTrainBDT(angularJetInfo_data, angularJetInfo_labels)\n",
    "\n",
    "# *** 3. Make mix of dihiggs and QCD for anglular+momentum basis jet info (pt, eta, phi, mass, E, px, py, pz)\n",
    "jetVariables = ['pt', 'eta', 'phi', 'mass', 'px', 'py', 'pz', 'energy']\n",
    "variables_allJetInfo = ['jet{0}_{1}'.format(iJetLabel, iJetVariable) for iJetLabel in jetLabels for iJetVariable in jetVariables]\n",
    "allJetInfo_data, allJetInfo_labels, hh_allJetInfo_data, hh_allJetInfo_labels, qcd_allJetInfo_data, qcd_allJetInfo_labels = makeEqualSamplesWithUserVariables(hh_raw, qcd_raw, variables_allJetInfo, nEventsToTrain) \n",
    "model_allJetInfo = setupAndTrainBDT(allJetInfo_data, allJetInfo_labels)\n",
    "\n",
    "# *** 4. Make mix of dihiggs and QCD for all variables\n",
    "variables_all = list(hh_raw.columns)\n",
    "variables_all.remove('isSignal') # don't want to pass answer to as part of data\n",
    "all_data, all_labels, hh_all_data, hh_all_labels, qcd_all_data, qcd_all_labels = makeEqualSamplesWithUserVariables(hh_raw, qcd_raw, variables_all, nEventsToTrain) \n",
    "model_all = setupAndTrainBDT(all_data, all_labels)\n",
    "\n",
    "# *** 5. Make mix of dihiggs and QCD for h1, h2, hh deltaR variables\n",
    "variables_deltaROnly = ['deltaR(h1, h2)', 'deltaR(h1 jets)', 'deltaR(h2 jets)']\n",
    "deltaROnly_data, deltaROnly_labels, hh_deltaROnly_data, hh_deltaROnly_labels, qcd_deltaROnly_data, qcd_deltaROnly_labels = makeEqualSamplesWithUserVariables(hh_raw, qcd_raw, variables_deltaROnly, nEventsToTrain) \n",
    "model_deltaROnly = setupAndTrainBDT(deltaROnly_data, deltaROnly_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   19.0s\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=4)]: Done 144 out of 144 | elapsed:  1.7min finished\n",
      "/home/btannenw/.local/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/btannenw/.local/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eta': 0.05, 'max_depth': 6, 'num_class': 2, 'objective': 'multi:softprob'} <class 'dict'>\n",
      "Precision = 0.743121970161085\n",
      "Recall = 0.7418482082964419\n",
      "Accuracy = 0.7406666666666667\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed:  7.6min\n",
      "[Parallel(n_jobs=4)]: Done 144 out of 144 | elapsed:  9.2min finished\n",
      "/home/btannenw/.local/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/btannenw/.local/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eta': 0.05, 'max_depth': 5, 'num_class': 2, 'objective': 'multi:softprob'} <class 'dict'>\n",
      "Precision = 0.5999019083421219\n",
      "Recall = 0.5971410637519361\n",
      "Accuracy = 0.5966666666666667\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:  2.9min\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed: 12.6min\n",
      "[Parallel(n_jobs=4)]: Done 144 out of 144 | elapsed: 15.0min finished\n",
      "/home/btannenw/.local/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/btannenw/.local/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eta': 0.05, 'max_depth': 8, 'num_class': 2, 'objective': 'multi:softprob'} <class 'dict'>\n",
      "Precision = 0.6055721058363764\n",
      "Recall = 0.605480923828223\n",
      "Accuracy = 0.6056666666666667\n",
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   18.0s\n",
      "[Parallel(n_jobs=4)]: Done 120 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=4)]: Done 144 out of 144 | elapsed:  1.8min finished\n",
      "/home/btannenw/.local/lib/python3.7/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/home/btannenw/.local/lib/python3.7/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eta': 0.05, 'max_depth': 3, 'num_class': 2, 'objective': 'multi:softprob'} <class 'dict'>\n",
      "Precision = 0.6847717699531088\n",
      "Recall = 0.6820208598066395\n",
      "Accuracy = 0.6838333333333333\n"
     ]
    }
   ],
   "source": [
    "#try grid\n",
    "grid_massesOnly = setupGridAndTrainBDT('massesOnly', massesOnly_data, massesOnly_labels, _saveModel=True)\n",
    "grid_angularJetInfo = setupGridAndTrainBDT('angularJetInfo', angularJetInfo_data, angularJetInfo_labels, _saveModel=True)\n",
    "grid_allJetInfo = setupGridAndTrainBDT('allJetInfo', allJetInfo_data, allJetInfo_labels, _saveModel=True)\n",
    "grid_deltaROnly = setupGridAndTrainBDT('deltaROnly', deltaROnly_data, deltaROnly_labels, _saveModel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_all = setupGridAndTrainBDT('allVariables', all_data, all_labels, _saveModel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBDTOutputAndTree(grid_massesOnly, 'Masses Only', hh_massesOnly_data, hh_massesOnly_labels, qcd_massesOnly_data, qcd_massesOnly_labels, _savePlots=True)\n",
    "plotBDTOutputAndTree(grid_angularJetInfo, 'Position Jet Info', hh_angularJetInfo_data, hh_angularJetInfo_labels, qcd_angularJetInfo_data, qcd_angularJetInfo_labels, _savePlots=True)\n",
    "plotBDTOutputAndTree(grid_allJetInfo, 'All Jet Info', hh_allJetInfo_data, hh_allJetInfo_labels, qcd_allJetInfo_data, qcd_allJetInfo_labels, _savePlots=True)\n",
    "plotBDTOutputAndTree(grid_deltaROnly, 'Delta R Only', hh_deltaROnly_data, hh_deltaROnly_labels, qcd_deltaROnly_data, qcd_deltaROnly_labels, _savePlots=True)\n",
    "plotBDTOutputAndTree(grid_all, 'All Variables', hh_all_data, hh_all_labels, qcd_all_data, qcd_all_labels, _savePlots=True)\n",
    "#plotBDTOutputAndTree(model_all, 'All Variables', hh_all_data, hh_all_labels, qcd_all_data, qcd_all_labels, _savePlots=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TO-DO\n",
    "[X] Grid optimization\n",
    "[X] Train with all variables\n",
    "3) isolate most important values \n",
    "[X] understand meaning of F score\n",
    "[X] slides\n",
    "6) some significance values for comparing with rectangular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** 5. Make mix of dihiggs and QCD for top-11 optimal variables\n",
    "variables_top11 = ['deltaR(h1, h2)', 'deltaR(h1 jets)', 'deltaR(h2 jets)', 'hh_mass', 'h1_mass', 'h2_mass','hh_pt', 'h1_pt', 'h2_pt', 'scalarHT', 'jet4_pt']\n",
    "top11_data, top11_labels, hh_top11_data, hh_top11_labels, qcd_top11_data, qcd_top11_labels = makeEqualSamplesWithUserVariables(hh_raw, qcd_raw, variables_top11, nEventsToTrain) \n",
    "model_top11 = setupAndTrainBDT(top11_data, top11_labels, _maxDepth=5, _eta=0.3, _modelName='top11', _saveModel=True)\n",
    "plotBDTOutputAndTree(model_top11, 'Top11', hh_top11_data, hh_top11_labels, qcd_top11_data, qcd_top11_labels, _savePlots=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** 5. Make mix of dihiggs and QCD for top-10 optimal variables\n",
    "variables_top10 = ['deltaR(h1, h2)', 'deltaR(h1 jets)', 'deltaR(h2 jets)', 'hh_mass', 'h1_mass', 'h2_mass','hh_pt', 'h1_pt', 'h2_pt', 'scalarHT']\n",
    "top10_data, top10_labels, hh_top10_data, hh_top10_labels, qcd_top10_data, qcd_top10_labels = makeEqualSamplesWithUserVariables(hh_raw, qcd_raw, variables_top10, nEventsToTrain) \n",
    "model_top10 = setupAndTrainBDT(top10_data, top10_labels, _maxDepth=7, _eta=0.3, _modelName='top10', _saveModel=True)\n",
    "plotBDTOutputAndTree(model_top10, 'top10', hh_top10_data, hh_top10_labels, qcd_top10_data, qcd_top10_labels, _savePlots=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** 5. Make mix of dihiggs and QCD for top-7 optimal variables\n",
    "variables_top7 = ['deltaR(h1, h2)', 'deltaR(h1 jets)', 'deltaR(h2 jets)', 'hh_mass', 'h1_mass', 'h2_mass', 'scalarHT']\n",
    "top7_data, top7_labels, hh_top7_data, hh_top7_labels, qcd_top7_data, qcd_top7_labels = makeEqualSamplesWithUserVariables(hh_raw, qcd_raw, variables_top7, nEventsToTrain) \n",
    "model_top7 = setupAndTrainBDT(top7_data, top7_labels, _maxDepth=6, _eta=0.3, _modelName='top7', _saveModel=True)\n",
    "plotBDTOutputAndTree(model_top7, 'top7', hh_top7_data, hh_top7_labels, qcd_top7_data, qcd_top7_labels, _savePlots=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *** 5. Make mix of dihiggs and QCD for top-6 optimal variables\n",
    "variables_top6 = ['deltaR(h1, h2)', 'deltaR(h1 jets)', 'deltaR(h2 jets)', 'h1_mass', 'h2_mass', 'scalarHT']\n",
    "top6_data, top6_labels, hh_top6_data, hh_top6_labels, qcd_top6_data, qcd_top6_labels = makeEqualSamplesWithUserVariables(hh_raw, qcd_raw, variables_top6, nEventsToTrain) \n",
    "model_top6 = setupAndTrainBDT(top6_data, top6_labels, _maxDepth=5, _eta=0.3, _modelName='top6', _saveModel=True)\n",
    "plotBDTOutputAndTree(model_top6, 'top6', hh_top6_data, hh_top6_labels, qcd_top6_data, qcd_top6_labels, _savePlots=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_all = setupGridAndTrainBDT('top10', top10_data, top10_labels, _saveModel=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBDTOutputAndTree(grid_all, 'top10', hh_top10_data, hh_top10_labels, qcd_top10_data, qcd_top10_labels, _savePlots=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotBDTOutputAndTree(model_top10, 'top10', hh_top10_data, hh_top10_labels, qcd_top10_data, qcd_top10_labels, _savePlots=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_top10_2 = setupAndTrainBDT(top10_data, top10_labels, _maxDepth=8, _eta=0.3)\n",
    "plotBDTOutputAndTree(model_top10_2, 'top10', hh_top10_data, hh_top10_labels, qcd_top10_data, qcd_top10_labels, _savePlots=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
