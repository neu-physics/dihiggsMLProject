{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from https://github.com/yunjey/pytorch-tutorial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.utils.data as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = 12       # The image size = 28 x 28 = 784\n",
    "# hidden_size = 500      # The number of nodes at the hidden layer\n",
    "# num_classes = 2       # The number of output classes. In this case, from 0 to 9\n",
    "# num_epochs = 5         # The number of times entire dataset is trained\n",
    "# batch_size = 100       # The size of input data took for one iteration\n",
    "# learning_rate = 0.000001  # The speed of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1703 rows of qcd data\n",
      "4605 rows of dihiggs data\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset\n",
    "qcd_raw = pd.read_csv('../HiggsReconstruction/EventPlotting/qcd_outputDataForLearning.csv')\n",
    "hh_raw = pd.read_csv('../HiggsReconstruction/EventPlotting/dihiggs_outputDataForLearning.csv')\n",
    "\n",
    "qcd_raw.head()\n",
    "print(len(qcd_raw), \"rows of qcd data\")\n",
    "hh_raw.head()\n",
    "print(len(hh_raw), \"rows of dihiggs data\")\n",
    "\n",
    "# Make higgs and qcd sets from raw data\n",
    "# hh_all = hh_raw[['h1_mass', 'h2_mass', 'deltaR(h1 jets)', 'deltaR(h2 jets)', 'jet1_pz', 'jet2_pz', 'jet3_pz', 'jet4_pz', 'jet1_energy', 'jet2_energy', 'jet3_energy', 'jet4_energy']]\n",
    "# qcd = qcd_raw[['h1_mass', 'h2_mass', 'deltaR(h1 jets)', 'deltaR(h2 jets)', 'jet1_pz', 'jet2_pz', 'jet3_pz', 'jet4_pz', 'jet1_energy', 'jet2_energy', 'jet3_energy', 'jet4_energy']]\n",
    "hh_all = hh_raw[['deltaR(h1 jets)', 'deltaR(h2 jets)']]\n",
    "qcd = qcd_raw[['deltaR(h1 jets)', 'deltaR(h2 jets)']]\n",
    "n_factors = np.shape(hh_all)[1]\n",
    "print(n_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.70037731 0.9287528  1.         0.        ]\n",
      " [0.35012013 0.49221131 0.         1.        ]\n",
      " [0.05149961 0.24521989 1.         0.        ]\n",
      " [0.81105927 0.400845   1.         0.        ]]\n",
      "[[1. 0.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "hh_all = np.array(hh_all)\n",
    "qcd = np.array(qcd)\n",
    "\n",
    "# add labels to di-higgs\n",
    "hh_all=hh_all[:,:n_factors]\n",
    "# # print(hh[0:3])\n",
    "hh_labels= np.zeros((len(hh_all),1))\n",
    "hh_labels = hh_labels+1\n",
    "#a = hh[:len(hh)]\n",
    "# print(a.shape)\n",
    "hh_all[:,0] = np.random.rand(np.shape(hh_all)[0])\n",
    "hh_all = np.append(hh_all, hh_labels, axis=1)\n",
    "hh_all = np.append(hh_all, 1-hh_labels, axis=1)## hh qcd labels \n",
    "\n",
    "\n",
    "# print(hh.shape)\n",
    "# print(hh[0:3])\n",
    "\n",
    "# add labels to qcd\n",
    "qcd=qcd[:,:n_factors]\n",
    "# print(hh[0:3])\n",
    "qcd_labels= np.zeros((len(qcd),1))\n",
    "#a = hh[:len(hh)]\n",
    "# print(a.shape)\n",
    "# qcd hh labels \n",
    "qcd[:, 0] = -1 * np.random.rand(np.shape(qcd)[0])\n",
    "qcd = np.append(qcd, qcd_labels, axis=1) \n",
    "qcd = np.append(qcd, 1-qcd_labels, axis=1)# qcd qcd labels\n",
    "\n",
    "\n",
    "# use this for dummy variables\n",
    "hh_all[:,0] = np.random.rand(np.shape(hh_all)[0])\n",
    "hh_all[:,1] = np.random.rand(np.shape(hh_all)[0])\n",
    "# qcd[:, 0] = -1 * np.random.rand(np.shape(qcd)[0])\n",
    "# qcd[:, 1] = -1 * np.random.rand(np.shape(qcd)[0])\n",
    "# qcd[:, 0] = np.random.rand(np.shape(qcd)[0])\n",
    "# qcd[:, 1] = np.random.rand(np.shape(qcd)[0])\n",
    "qcd[:, 0] = hh_all[:len(qcd),0]\n",
    "qcd[:, 1] = hh_all[:len(qcd),1]\n",
    "\n",
    "\n",
    "# select a quarter of hh events so that the set is half and half\n",
    "# we shuffle the list first to take a random 1/4. this means we have a different dataset every time\n",
    "# np.random.seed(0)\n",
    "# np.random.shuffle(hh_all) \n",
    "hh = hh_all[0:len(qcd)]\n",
    "# print(hh[:4])\n",
    "# print(qcd[:4])\n",
    "\n",
    "all_data = np.append(hh,qcd, axis=0) \n",
    "all_data[:n_factors,:]\n",
    "\n",
    "np.random.seed(0)\n",
    "for i in range (4): # shuffle 4 times\n",
    "    np.random.shuffle(all_data) \n",
    "print(all_data[:4])\n",
    "all_labels = all_data[:,n_factors:]\n",
    "# for testing model resilience\n",
    "# for i in range(2):\n",
    "#     np.random.shuffle(all_labels)\n",
    "all_data = all_data[:,:n_factors]\n",
    "# print(all_data[:4])\n",
    "print(all_labels[:4])\n",
    "# print(test_data)\n",
    "# print(len(all_data))\n",
    "# print(all_labels)\n",
    "\n",
    "input_size = n_factors       # The image size = 28 x 28 = 784\n",
    "hidden_size = 500      # The number of nodes at the hidden layer\n",
    "num_classes = all_labels.shape[1]       # The number of output classes. In this case, from 0 to 9\n",
    "num_epochs = 5         # The number of times entire dataset is trained\n",
    "batch_size = 100       # The size of input data took for one iteration\n",
    "learning_rate = 0.001  # The speed of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.70042873 0.92880387]\n",
      " [0.35014584 0.49223837]\n",
      " [0.05150339 0.24523337]\n",
      " [0.81111881 0.40086704]]\n"
     ]
    }
   ],
   "source": [
    "# scale the data by dividing it by the max value of each\n",
    "for i in range(np.shape(all_data)[1]):\n",
    "    all_data[:,i] = np.true_divide(all_data[:,i], np.max(all_data[:,i]))\n",
    "print(all_data[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x11a08b8d0>\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(all_data, all_labels, train_size=0.3, test_size=0.5, random_state=42)\n",
    "\n",
    "train_data = torch.stack([torch.Tensor(i) for i in train_data])\n",
    "train_labels = torch.stack([torch.Tensor(i) for i in train_labels])\n",
    "test_data = torch.stack([torch.Tensor(i) for i in test_data])\n",
    "test_labels = torch.stack([torch.Tensor(i) for i in test_labels])\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = utils.TensorDataset(train_data, train_labels)\n",
    "\n",
    "test_dataset = utils.TensorDataset(test_data, test_labels)\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x11a0a1780>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader = utils.DataLoader(train_dataset)\n",
    "\n",
    "test_loader = utils.DataLoader(test_dataset)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<enumerate object at 0x11a0a4f30>\n",
      "0 [tensor([[0.5523, 0.1416]]), tensor([[1., 0.]])]\n",
      "1 [tensor([[0.9918, 0.0296]]), tensor([[0., 1.]])]\n",
      "2 [tensor([[0.1546, 0.9431]]), tensor([[0., 1.]])]\n",
      "3 [tensor([[0.3340, 0.8230]]), tensor([[0., 1.]])]\n",
      "4 [tensor([[0.3084, 0.7789]]), tensor([[1., 0.]])]\n",
      "5 [tensor([[0.9753, 0.7578]]), tensor([[1., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "print(enumerate(train_loader))\n",
    "for i, e in enumerate(train_loader):\n",
    "    print(i, e)\n",
    "    if(i>4):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()                    # Inherited from the parent class nn.Module\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer: 784 (input data) -> 500 (hidden node)\n",
    "        self.relu = nn.ReLU()                          # Non-Linear ReLU Layer: max(0,x)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # 2nd Full-Connected Layer: 500 (hidden node) -> 10 (output class)\n",
    "    \n",
    "    def forward(self, x):                              # Forward pass: stacking each layer together\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size, hidden_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.cuda()    # You can comment out this line to disable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/10], Loss: 0.6699\n",
      "Epoch [1/5], Step [200/10], Loss: 0.6800\n",
      "Epoch [1/5], Step [300/10], Loss: 0.9490\n",
      "Epoch [1/5], Step [400/10], Loss: 1.3598\n",
      "Epoch [1/5], Step [500/10], Loss: 0.6646\n",
      "Epoch [1/5], Step [600/10], Loss: 0.6152\n",
      "Epoch [1/5], Step [700/10], Loss: 0.5246\n",
      "Epoch [1/5], Step [800/10], Loss: 0.6436\n",
      "Epoch [1/5], Step [900/10], Loss: 1.3878\n",
      "Epoch [1/5], Step [1000/10], Loss: 1.5333\n",
      "Epoch [2/5], Step [100/10], Loss: 0.8062\n",
      "Epoch [2/5], Step [200/10], Loss: 0.7949\n",
      "Epoch [2/5], Step [300/10], Loss: 0.9589\n",
      "Epoch [2/5], Step [400/10], Loss: 1.2454\n",
      "Epoch [2/5], Step [500/10], Loss: 0.6573\n",
      "Epoch [2/5], Step [600/10], Loss: 0.6173\n",
      "Epoch [2/5], Step [700/10], Loss: 0.4721\n",
      "Epoch [2/5], Step [800/10], Loss: 0.6579\n",
      "Epoch [2/5], Step [900/10], Loss: 1.2035\n",
      "Epoch [2/5], Step [1000/10], Loss: 1.4511\n",
      "Epoch [3/5], Step [100/10], Loss: 0.7677\n",
      "Epoch [3/5], Step [200/10], Loss: 0.8147\n",
      "Epoch [3/5], Step [300/10], Loss: 0.9768\n",
      "Epoch [3/5], Step [400/10], Loss: 1.1707\n",
      "Epoch [3/5], Step [500/10], Loss: 0.6545\n",
      "Epoch [3/5], Step [600/10], Loss: 0.6175\n",
      "Epoch [3/5], Step [700/10], Loss: 0.4483\n",
      "Epoch [3/5], Step [800/10], Loss: 0.6631\n",
      "Epoch [3/5], Step [900/10], Loss: 1.0651\n",
      "Epoch [3/5], Step [1000/10], Loss: 1.3614\n",
      "Epoch [4/5], Step [100/10], Loss: 0.7335\n",
      "Epoch [4/5], Step [200/10], Loss: 0.8157\n",
      "Epoch [4/5], Step [300/10], Loss: 0.9871\n",
      "Epoch [4/5], Step [400/10], Loss: 1.1162\n",
      "Epoch [4/5], Step [500/10], Loss: 0.6513\n",
      "Epoch [4/5], Step [600/10], Loss: 0.6199\n",
      "Epoch [4/5], Step [700/10], Loss: 0.4409\n",
      "Epoch [4/5], Step [800/10], Loss: 0.6703\n",
      "Epoch [4/5], Step [900/10], Loss: 0.9714\n",
      "Epoch [4/5], Step [1000/10], Loss: 1.2751\n",
      "Epoch [5/5], Step [100/10], Loss: 0.7067\n",
      "Epoch [5/5], Step [200/10], Loss: 0.8110\n",
      "Epoch [5/5], Step [300/10], Loss: 0.9918\n",
      "Epoch [5/5], Step [400/10], Loss: 1.0690\n",
      "Epoch [5/5], Step [500/10], Loss: 0.6436\n",
      "Epoch [5/5], Step [600/10], Loss: 0.6257\n",
      "Epoch [5/5], Step [700/10], Loss: 0.4392\n",
      "Epoch [5/5], Step [800/10], Loss: 0.6801\n",
      "Epoch [5/5], Step [900/10], Loss: 0.8977\n",
      "Epoch [5/5], Step [1000/10], Loss: 1.1967\n",
      "1021\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in enumerate(train_loader):   # Load a batch of images with its (index, data, class)\n",
    "        data = Variable(data)       # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        outputs = net(data)                             # Forward pass: compute the output class given a image\n",
    "        labels = labels.long()\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimizer.step()                                  # Optimizer: update the weights of hidden nodes\n",
    "        \n",
    "        if (i+1) % 100 == 0:                              # Logging\n",
    "#             print(\"SMH\")\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data.item()))\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10K test images: 49 %\n",
      "tensor(846)\n",
      "1703\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data, labels in test_loader:\n",
    "    data = Variable(data)\n",
    "    outputs = net(data)\n",
    "    _, predicted = torch.max(outputs.data, 1)  # Choose the best class from the output: The class with the best score\n",
    "    total += labels.size(0)                    # Increment the total count\n",
    "    labels = labels.long()\n",
    "    correct += (predicted == torch.max(labels, 1)[1]).sum()     # Increment the correct count\n",
    "    \n",
    "print('Accuracy of the network on the 10K test images: %d %%' % (100 * correct / total))\n",
    "print(correct)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eulerspython",
   "language": "python",
   "name": "eulerspython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
