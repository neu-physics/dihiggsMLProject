{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from https://github.com/yunjey/pytorch-tutorial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.utils.data as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = 12       # The image size = 28 x 28 = 784\n",
    "# hidden_size = 500      # The number of nodes at the hidden layer\n",
    "# num_classes = 2       # The number of output classes. In this case, from 0 to 9\n",
    "# num_epochs = 5         # The number of times entire dataset is trained\n",
    "# batch_size = 100       # The size of input data took for one iteration\n",
    "# learning_rate = 0.001  # The speed of convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1703 rows of qcd data\n",
      "4605 rows of dihiggs data\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset\n",
    "qcd_raw = pd.read_csv('../HiggsReconstruction/EventPlotting/qcd_outputDataForLearning.csv')\n",
    "hh_raw = pd.read_csv('../HiggsReconstruction/EventPlotting/dihiggs_outputDataForLearning.csv')\n",
    "\n",
    "qcd_raw.head()\n",
    "print(len(qcd_raw), \"rows of qcd data\")\n",
    "hh_raw.head()\n",
    "print(len(hh_raw), \"rows of dihiggs data\")\n",
    "\n",
    "# Make higgs and qcd sets from raw data\n",
    "# hh_all = hh_raw[['h1_mass', 'h2_mass', 'deltaR(h1 jets)', 'deltaR(h2 jets)', 'jet1_pz', 'jet2_pz', 'jet3_pz', 'jet4_pz', 'jet1_energy', 'jet2_energy', 'jet3_energy', 'jet4_energy']]\n",
    "# qcd = qcd_raw[['h1_mass', 'h2_mass', 'deltaR(h1 jets)', 'deltaR(h2 jets)', 'jet1_pz', 'jet2_pz', 'jet3_pz', 'jet4_pz', 'jet1_energy', 'jet2_energy', 'jet3_energy', 'jet4_energy']]\n",
    "\n",
    "#same as ruhi\n",
    "# hh_all = hh_raw[['hh_mass', 'h1_mass', 'h2_mass', 'deltaR(h1 jets)', 'deltaR(h2 jets)', 'deltaPhi(h1, h2)', 'deltaPhi(h1 jets)', 'deltaPhi(h2 jets)']]\n",
    "# qcd = qcd_raw[['hh_mass', 'h1_mass', 'h2_mass', 'deltaR(h1 jets)', 'deltaR(h2 jets)', 'deltaPhi(h1, h2)', 'deltaPhi(h1 jets)', 'deltaPhi(h2 jets)']]\n",
    "\n",
    "hh_all = hh_raw[['deltaR(h1 jets)', 'deltaR(h2 jets)']]\n",
    "qcd = qcd_raw[['deltaR(h1 jets)', 'deltaR(h2 jets)']]\n",
    "n_factors = np.shape(hh_all)[1]\n",
    "print(n_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79127593 3.04517197 1.         0.        ]\n",
      " [0.00430714 1.1183898  1.         0.        ]\n",
      " [0.80359554 1.15238492 1.         0.        ]\n",
      " [0.36926893 2.09055659 1.         0.        ]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "hh_all = np.array(hh_all)\n",
    "qcd = np.array(qcd)\n",
    "\n",
    "# add labels to di-higgs\n",
    "hh_all=hh_all[:,:n_factors]\n",
    "# # print(hh[0:3])\n",
    "hh_labels= np.zeros((len(hh_all),1))\n",
    "hh_labels = hh_labels+1\n",
    "#a = hh[:len(hh)]\n",
    "# print(a.shape)\n",
    "hh_all[:,0] = np.random.rand(np.shape(hh_all)[0])\n",
    "hh_all = np.append(hh_all, hh_labels, axis=1)\n",
    "hh_all = np.append(hh_all, 1-hh_labels, axis=1)## hh qcd labels \n",
    "\n",
    "\n",
    "# print(hh.shape)\n",
    "# print(hh[0:3])\n",
    "\n",
    "# add labels to qcd\n",
    "qcd=qcd[:,:n_factors]\n",
    "# print(hh[0:3])\n",
    "qcd_labels= np.zeros((len(qcd),1))\n",
    "#a = hh[:len(hh)]\n",
    "# print(a.shape)\n",
    "# qcd hh labels \n",
    "qcd[:, 0] = -1 * np.random.rand(np.shape(qcd)[0])\n",
    "qcd = np.append(qcd, qcd_labels, axis=1) \n",
    "qcd = np.append(qcd, 1-qcd_labels, axis=1)# qcd qcd labels\n",
    "\n",
    "\n",
    "# use this for dummy variables\n",
    "# hh_all[:,0] = np.random.rand(np.shape(hh_all)[0])\n",
    "# hh_all[:,1] = np.random.rand(np.shape(hh_all)[0])\n",
    "# qcd[:, 0] = -1 * np.random.rand(np.shape(qcd)[0])\n",
    "# qcd[:, 1] = -1 * np.random.rand(np.shape(qcd)[0])\n",
    "# qcd[:, 0] = np.random.rand(np.shape(qcd)[0])\n",
    "# qcd[:, 1] = np.random.rand(np.shape(qcd)[0])\n",
    "# qcd[:, 0] = hh_all[:len(qcd),0] -1#-.5\n",
    "# qcd[:, 1] = hh_all[:len(qcd),1] -1#-.5\n",
    "\n",
    "\n",
    "# select a quarter of hh events so that the set is half and half\n",
    "# we shuffle the list first to take a random 1/4. this means we have a different dataset every time\n",
    "# np.random.seed(0)\n",
    "# np.random.shuffle(hh_all) \n",
    "hh = hh_all[0:len(qcd)]\n",
    "# print(hh[:4])\n",
    "# print(qcd[:4])\n",
    "\n",
    "all_data = np.append(hh,qcd, axis=0) \n",
    "all_data[:n_factors,:]\n",
    "\n",
    "np.random.seed(0)\n",
    "# for i in range (4): # shuffle 4 times\n",
    "#     np.random.shuffle(all_data) \n",
    "print(all_data[:4])\n",
    "all_labels = all_data[:,n_factors:]\n",
    "# for testing model resilience\n",
    "# for i in range(2):\n",
    "#     np.random.shuffle(all_labels)\n",
    "all_data = all_data[:,:n_factors]\n",
    "# print(all_data[:4])\n",
    "print(all_labels[:4])\n",
    "# print(test_data)\n",
    "# print(len(all_data))\n",
    "# print(all_labels)\n",
    "\n",
    "input_size = n_factors       # The image size = 28 x 28 = 784\n",
    "hidden_size = 500      # The number of nodes at the hidden layer\n",
    "num_classes = all_labels.shape[1]       # The number of output classes. In this case, from 0 to 9\n",
    "num_epochs = 5         # The number of times entire dataset is trained\n",
    "batch_size = 100       # The size of input data took for one iteration\n",
    "learning_rate = 0.001  # The speed of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.79259147 0.60794872]\n",
      " [0.0043143  0.22327923]\n",
      " [0.80493156 0.23006613]\n",
      " [0.36988286 0.41736598]]\n"
     ]
    }
   ],
   "source": [
    "# scale the data by dividing it by the max value of each\n",
    "for i in range(np.shape(all_data)[1]):\n",
    "    all_data[:,i] = np.true_divide(all_data[:,i], np.max(all_data[:,i]))\n",
    "print(all_data[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(all_data, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x126977860>\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(all_data, all_labels, train_size=0.3, test_size=0.5, random_state=42)\n",
    "\n",
    "train_data = torch.stack([torch.Tensor(i) for i in train_data])\n",
    "train_labels = torch.stack([torch.Tensor(i) for i in train_labels])\n",
    "test_data = torch.stack([torch.Tensor(i) for i in test_data])\n",
    "test_labels = torch.stack([torch.Tensor(i) for i in test_labels])\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = utils.TensorDataset(train_data, train_labels)\n",
    "\n",
    "test_dataset = utils.TensorDataset(test_data, test_labels)\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x1268dc160>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader = utils.DataLoader(train_dataset)\n",
    "\n",
    "test_loader = utils.DataLoader(test_dataset)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<enumerate object at 0x1269865a0>\n",
      "0 [tensor([[0.6845, 0.6293]]), tensor([[1., 0.]])]\n",
      "1 [tensor([[-0.2378,  0.3372]]), tensor([[0., 1.]])]\n",
      "2 [tensor([[0.9765, 0.2406]]), tensor([[1., 0.]])]\n",
      "3 [tensor([[-0.0407,  0.6811]]), tensor([[0., 1.]])]\n",
      "4 [tensor([[-0.1019,  0.4398]]), tensor([[0., 1.]])]\n",
      "5 [tensor([[0.8043, 0.7800]]), tensor([[1., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "print(enumerate(train_loader))\n",
    "for i, e in enumerate(train_loader):\n",
    "    print(i, e)\n",
    "    if(i>4):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()                    # Inherited from the parent class nn.Module\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer: 784 (input data) -> 500 (hidden node)\n",
    "#         self.relu = nn.Softmax(dim=None)                          # Non-Linear ReLU Layer: max(0,x)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # 2nd Full-Connected Layer: 500 (hidden node) -> 10 (output class)\n",
    "    \n",
    "    def forward(self, x):                              # Forward pass: stacking each layer together\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size, hidden_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.cuda()    # You can comment out this line to disable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA SHAPE: torch.Size([1, 2])\n",
      "OUTPUT SHAPE: torch.Size([1, 2])\n",
      "Epoch [1/5], Step [100/10], Loss: 0.6423\n",
      "Epoch [1/5], Step [200/10], Loss: 0.0223\n",
      "Epoch [1/5], Step [300/10], Loss: 0.0039\n",
      "Epoch [1/5], Step [400/10], Loss: 0.0002\n",
      "Epoch [1/5], Step [500/10], Loss: 0.2198\n",
      "Epoch [1/5], Step [600/10], Loss: 0.0000\n",
      "Epoch [1/5], Step [700/10], Loss: 0.0000\n",
      "Epoch [1/5], Step [800/10], Loss: 0.4664\n",
      "Epoch [1/5], Step [900/10], Loss: 0.0072\n",
      "Epoch [1/5], Step [1000/10], Loss: 0.0083\n",
      "DATA SHAPE: torch.Size([1, 2])\n",
      "OUTPUT SHAPE: torch.Size([1, 2])\n",
      "Epoch [2/5], Step [100/10], Loss: 0.7983\n",
      "Epoch [2/5], Step [200/10], Loss: 0.0000\n",
      "Epoch [2/5], Step [300/10], Loss: 0.0000\n",
      "Epoch [2/5], Step [400/10], Loss: 0.0000\n",
      "Epoch [2/5], Step [500/10], Loss: 0.0411\n",
      "Epoch [2/5], Step [600/10], Loss: 0.0000\n",
      "Epoch [2/5], Step [700/10], Loss: 0.0000\n",
      "Epoch [2/5], Step [800/10], Loss: 0.2406\n",
      "Epoch [2/5], Step [900/10], Loss: 0.0003\n",
      "Epoch [2/5], Step [1000/10], Loss: 0.0005\n",
      "DATA SHAPE: torch.Size([1, 2])\n",
      "OUTPUT SHAPE: torch.Size([1, 2])\n",
      "Epoch [3/5], Step [100/10], Loss: 0.8983\n",
      "Epoch [3/5], Step [200/10], Loss: 0.0000\n",
      "Epoch [3/5], Step [300/10], Loss: 0.0000\n",
      "Epoch [3/5], Step [400/10], Loss: 0.0000\n",
      "Epoch [3/5], Step [500/10], Loss: 0.0098\n",
      "Epoch [3/5], Step [600/10], Loss: 0.0000\n",
      "Epoch [3/5], Step [700/10], Loss: 0.0000\n",
      "Epoch [3/5], Step [800/10], Loss: 0.1451\n",
      "Epoch [3/5], Step [900/10], Loss: 0.0000\n",
      "Epoch [3/5], Step [1000/10], Loss: 0.0001\n",
      "DATA SHAPE: torch.Size([1, 2])\n",
      "OUTPUT SHAPE: torch.Size([1, 2])\n",
      "Epoch [4/5], Step [100/10], Loss: 0.7150\n",
      "Epoch [4/5], Step [200/10], Loss: 0.0000\n",
      "Epoch [4/5], Step [300/10], Loss: 0.0000\n",
      "Epoch [4/5], Step [400/10], Loss: 0.0000\n",
      "Epoch [4/5], Step [500/10], Loss: 0.0028\n",
      "Epoch [4/5], Step [600/10], Loss: 0.0000\n",
      "Epoch [4/5], Step [700/10], Loss: 0.0000\n",
      "Epoch [4/5], Step [800/10], Loss: 0.0905\n",
      "Epoch [4/5], Step [900/10], Loss: 0.0000\n",
      "Epoch [4/5], Step [1000/10], Loss: 0.0000\n",
      "DATA SHAPE: torch.Size([1, 2])\n",
      "OUTPUT SHAPE: torch.Size([1, 2])\n",
      "Epoch [5/5], Step [100/10], Loss: 0.4890\n",
      "Epoch [5/5], Step [200/10], Loss: 0.0000\n",
      "Epoch [5/5], Step [300/10], Loss: 0.0000\n",
      "Epoch [5/5], Step [400/10], Loss: 0.0000\n",
      "Epoch [5/5], Step [500/10], Loss: 0.0008\n",
      "Epoch [5/5], Step [600/10], Loss: 0.0000\n",
      "Epoch [5/5], Step [700/10], Loss: 0.0000\n",
      "Epoch [5/5], Step [800/10], Loss: 0.0505\n",
      "Epoch [5/5], Step [900/10], Loss: 0.0000\n",
      "Epoch [5/5], Step [1000/10], Loss: 0.0000\n",
      "1021\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in enumerate(train_loader):   # Load a batch of images with its (index, data, class)\n",
    "        data = Variable(data)       # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        if(i==0):\n",
    "            print(\"DATA SHAPE:\", data.shape)\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        outputs = net(data)                             # Forward pass: compute the output class given a image\n",
    "        if(i==0):\n",
    "            print(\"OUTPUT SHAPE:\", outputs.shape)\n",
    "        labels = labels.long()\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimizer.step()                                  # Optimizer: update the weights of hidden nodes\n",
    "        \n",
    "        if (i+1) % 100 == 0:                              # Logging\n",
    "#             print(\"SMH\")\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data.item()))\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10K test images: 99 %\n",
      "correct: 1696\n",
      "total: 1703\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data, labels in test_loader:\n",
    "    data = Variable(data)\n",
    "    outputs = net(data)\n",
    "    _, predicted = torch.max(outputs.data, 1)  # Choose the best class from the output: The class with the best score\n",
    "    total += labels.size(0)                    # Increment the total count\n",
    "    labels = labels.long()\n",
    "    correct += (predicted == torch.max(labels, 1)[1]).sum()     # Increment the correct count\n",
    "    \n",
    "print('Accuracy of the network on the 10K test images: %d %%' % (100 * correct / total))\n",
    "print(\"correct:\", correct.item())\n",
    "print(\"total:\", total)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eulerspython",
   "language": "python",
   "name": "eulerspython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
