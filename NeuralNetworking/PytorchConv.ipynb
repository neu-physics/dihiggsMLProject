{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from https://github.com/yunjey/pytorch-tutorial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.utils.data as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = 12       # The image size = 28 x 28 = 784\n",
    "# hidden_size = 500      # The number of nodes at the hidden layer\n",
    "# num_classes = 2       # The number of output classes. In this case, from 0 to 9\n",
    "# num_epochs = 5         # The number of times entire dataset is trained\n",
    "# batch_size = 100       # The size of input data took for one iteration\n",
    "# learning_rate = 0.001  # The speed of convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1703 rows of qcd data\n",
      "4605 rows of dihiggs data\n",
      "8\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset\n",
    "qcd_raw = pd.read_csv('../HiggsReconstruction/EventPlotting/qcd_outputDataForLearning.csv')\n",
    "hh_raw = pd.read_csv('../HiggsReconstruction/EventPlotting/dihiggs_outputDataForLearning.csv')\n",
    "\n",
    "qcd_raw.head()\n",
    "print(len(qcd_raw), \"rows of qcd data\")\n",
    "hh_raw.head()\n",
    "print(len(hh_raw), \"rows of dihiggs data\")\n",
    "\n",
    "# Make higgs and qcd sets from raw data\n",
    "# hh_all = hh_raw[['h1_mass', 'h2_mass', 'deltaR(h1 jets)', 'deltaR(h2 jets)', 'jet1_pz', 'jet2_pz', 'jet3_pz', 'jet4_pz', 'jet1_energy', 'jet2_energy', 'jet3_energy', 'jet4_energy']]\n",
    "# qcd = qcd_raw[['h1_mass', 'h2_mass', 'deltaR(h1 jets)', 'deltaR(h2 jets)', 'jet1_pz', 'jet2_pz', 'jet3_pz', 'jet4_pz', 'jet1_energy', 'jet2_energy', 'jet3_energy', 'jet4_energy']]\n",
    "\n",
    "#same as ruhi\n",
    "hh_all = hh_raw[['hh_mass', 'h1_mass', 'h2_mass', 'deltaR(h1 jets)', 'deltaR(h2 jets)', 'deltaPhi(h1, h2)', 'deltaPhi(h1 jets)', 'deltaPhi(h2 jets)']]\n",
    "qcd = qcd_raw[['hh_mass', 'h1_mass', 'h2_mass', 'deltaR(h1 jets)', 'deltaR(h2 jets)', 'deltaPhi(h1, h2)', 'deltaPhi(h1 jets)', 'deltaPhi(h2 jets)']]\n",
    "\n",
    "# hh_all = hh_raw[['deltaR(h1 jets)', 'deltaR(h2 jets)']]\n",
    "# qcd = qcd_raw[['deltaR(h1 jets)', 'deltaR(h2 jets)']]\n",
    "n_factors = np.shape(hh_all)[1]\n",
    "print(n_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.59318947e-02  1.48291836e+02  1.58296646e+02  2.39941158e+00\n",
      "   3.04517197e+00 -1.39974523e+00  2.26137894e+00 -2.73688633e+00\n",
      "   1.00000000e+00  0.00000000e+00]\n",
      " [ 6.76571469e-01  9.65536398e+01  5.85546013e+01  1.34373550e+00\n",
      "   1.11838980e+00 -2.62879683e+00  1.20060802e+00  1.10447589e+00\n",
      "   1.00000000e+00  0.00000000e+00]\n",
      " [ 2.12799951e-01  1.10271880e+02  1.30164227e+02  6.97377344e-01\n",
      "   1.15238492e+00 -1.70520791e+00 -6.44060135e-01  1.15179259e+00\n",
      "   1.00000000e+00  0.00000000e+00]\n",
      " [ 1.85365460e-01  1.79640526e+02  1.66136193e+02  2.05337635e+00\n",
      "   2.09055659e+00 -2.44151538e+00 -1.87211138e+00  1.42004728e+00\n",
      "   1.00000000e+00  0.00000000e+00]]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "hh_all = np.array(hh_all)\n",
    "qcd = np.array(qcd)\n",
    "\n",
    "# add labels to di-higgs\n",
    "hh_all=hh_all[:,:n_factors]\n",
    "# # print(hh[0:3])\n",
    "hh_labels= np.zeros((len(hh_all),1))\n",
    "hh_labels = hh_labels+1\n",
    "#a = hh[:len(hh)]\n",
    "# print(a.shape)\n",
    "hh_all[:,0] = np.random.rand(np.shape(hh_all)[0])\n",
    "hh_all = np.append(hh_all, hh_labels, axis=1)\n",
    "hh_all = np.append(hh_all, 1-hh_labels, axis=1)## hh qcd labels \n",
    "\n",
    "\n",
    "# print(hh.shape)\n",
    "# print(hh[0:3])\n",
    "\n",
    "# add labels to qcd\n",
    "qcd=qcd[:,:n_factors]\n",
    "# print(hh[0:3])\n",
    "qcd_labels= np.zeros((len(qcd),1))\n",
    "#a = hh[:len(hh)]\n",
    "# print(a.shape)\n",
    "# qcd hh labels \n",
    "qcd[:, 0] = -1 * np.random.rand(np.shape(qcd)[0])\n",
    "qcd = np.append(qcd, qcd_labels, axis=1) \n",
    "qcd = np.append(qcd, 1-qcd_labels, axis=1)# qcd qcd labels\n",
    "\n",
    "\n",
    "# use this for dummy variables\n",
    "# hh_all[:,0] = np.random.rand(np.shape(hh_all)[0])\n",
    "# hh_all[:,1] = np.random.rand(np.shape(hh_all)[0])\n",
    "# qcd[:, 0] = -1 * np.random.rand(np.shape(qcd)[0])\n",
    "# qcd[:, 1] = -1 * np.random.rand(np.shape(qcd)[0])\n",
    "# qcd[:, 0] = np.random.rand(np.shape(qcd)[0])\n",
    "# qcd[:, 1] = np.random.rand(np.shape(qcd)[0])\n",
    "# qcd[:, 0] = hh_all[:len(qcd),0] -1#-.5\n",
    "# qcd[:, 1] = hh_all[:len(qcd),1] -1#-.5\n",
    "\n",
    "\n",
    "# select a quarter of hh events so that the set is half and half\n",
    "# we shuffle the list first to take a random 1/4. this means we have a different dataset every time\n",
    "# np.random.seed(0)\n",
    "# np.random.shuffle(hh_all) \n",
    "hh = hh_all[0:len(qcd)]\n",
    "# print(hh[:4])\n",
    "# print(qcd[:4])\n",
    "\n",
    "all_data = np.append(hh,qcd, axis=0) \n",
    "all_data[:n_factors,:]\n",
    "\n",
    "np.random.seed(0)\n",
    "# for i in range (4): # shuffle 4 times\n",
    "#     np.random.shuffle(all_data) \n",
    "print(all_data[:4])\n",
    "all_labels = all_data[:,n_factors:]\n",
    "# for testing model resilience\n",
    "# for i in range(2):\n",
    "#     np.random.shuffle(all_labels)\n",
    "all_data = all_data[:,:n_factors]\n",
    "# print(all_data[:4])\n",
    "print(all_labels[:4])\n",
    "# print(test_data)\n",
    "# print(len(all_data))\n",
    "# print(all_labels)\n",
    "\n",
    "input_size = n_factors       # The image size = 28 x 28 = 784\n",
    "hidden_size = 500      # The number of nodes at the hidden layer\n",
    "num_classes = all_labels.shape[1]       # The number of output classes. In this case, from 0 to 9\n",
    "num_epochs = 5         # The number of times entire dataset is trained\n",
    "batch_size = 100       # The size of input data took for one iteration\n",
    "learning_rate = 0.001  # The speed of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01593693  0.20528419  0.26390849  0.43658628  0.60794872 -0.44564699\n",
      "   0.71985814 -0.87118638]\n",
      " [ 0.67678545  0.13366168  0.09762087  0.24450015  0.22327923 -0.83694901\n",
      "   0.38218603  0.35156898]\n",
      " [ 0.21286725  0.15265219  0.21700677  0.12689168  0.23006613 -0.54289934\n",
      "  -0.20502178  0.3666305 ]\n",
      " [ 0.18542409  0.24868099  0.2769784   0.37362324  0.41736598 -0.77732286\n",
      "  -0.59594373  0.45201945]]\n"
     ]
    }
   ],
   "source": [
    "# scale the data by dividing it by the max value of each\n",
    "for i in range(np.shape(all_data)[1]):\n",
    "    all_data[:,i] = np.true_divide(all_data[:,i], np.max(all_data[:,i]))\n",
    "print(all_data[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(all_data, bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.TensorDataset object at 0x11e156b00>\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(all_data, all_labels, train_size=0.3, test_size=0.5, random_state=42)\n",
    "\n",
    "train_data = torch.stack([torch.Tensor(i) for i in train_data])\n",
    "train_labels = torch.stack([torch.Tensor(i) for i in train_labels])\n",
    "test_data = torch.stack([torch.Tensor(i) for i in test_data])\n",
    "test_labels = torch.stack([torch.Tensor(i) for i in test_labels])\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = utils.TensorDataset(train_data, train_labels)\n",
    "\n",
    "test_dataset = utils.TensorDataset(test_data, test_labels)\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x1215b03c8>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_loader = utils.DataLoader(train_dataset)\n",
    "\n",
    "test_loader = utils.DataLoader(test_dataset)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<enumerate object at 0x1204a4120>\n",
      "0 [tensor([[ 0.7244,  0.4726,  0.5254,  0.5906,  0.6293,  0.6913, -0.9937, -0.9703]]), tensor([[1., 0.]])]\n",
      "1 [tensor([[-0.6916,  0.1496,  0.2398,  0.1998,  0.3372,  0.9316, -0.3328, -0.5155]]), tensor([[0., 1.]])]\n",
      "2 [tensor([[ 0.3793,  0.1374,  0.1940,  0.1896,  0.2406, -0.9624, -0.2967,  0.3297]]), tensor([[1., 0.]])]\n",
      "3 [tensor([[-0.9789,  0.3535,  0.3438,  0.6307,  0.6811, -0.9389, -0.8562, -0.5508]]), tensor([[0., 1.]])]\n",
      "4 [tensor([[-0.9326,  0.3576,  0.2657,  0.5276,  0.4398, -0.9080, -0.9216,  0.1880]]), tensor([[0., 1.]])]\n",
      "5 [tensor([[ 0.6748,  0.3276,  0.3842,  0.5811,  0.7800, -0.9923, -0.9717,  0.3890]]), tensor([[1., 0.]])]\n"
     ]
    }
   ],
   "source": [
    "print(enumerate(train_loader))\n",
    "for i, e in enumerate(train_loader):\n",
    "    print(i, e)\n",
    "    if(i>4):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()                    # Inherited from the parent class nn.Module\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer: 784 (input data) -> 500 (hidden node)\n",
    "#         self.relu = nn.Softmax(dim=None)                          # Non-Linear ReLU Layer: max(0,x)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # 2nd Full-Connected Layer: 500 (hidden node) -> 10 (output class)\n",
    "    \n",
    "    def forward(self, x):                              # Forward pass: stacking each layer together\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size, hidden_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.cuda()    # You can comment out this line to disable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/10], Loss: 0.5263\n",
      "Epoch [1/5], Step [200/10], Loss: 0.0138\n",
      "Epoch [1/5], Step [300/10], Loss: 0.0158\n",
      "Epoch [1/5], Step [400/10], Loss: 0.0004\n",
      "Epoch [1/5], Step [500/10], Loss: 0.0012\n",
      "Epoch [1/5], Step [600/10], Loss: 0.0001\n",
      "Epoch [1/5], Step [700/10], Loss: 0.0001\n",
      "Epoch [1/5], Step [800/10], Loss: 0.0003\n",
      "Epoch [1/5], Step [900/10], Loss: 0.0026\n",
      "Epoch [1/5], Step [1000/10], Loss: 0.0000\n",
      "Epoch [2/5], Step [100/10], Loss: 0.1847\n",
      "Epoch [2/5], Step [200/10], Loss: 0.0000\n",
      "Epoch [2/5], Step [300/10], Loss: 0.0000\n",
      "Epoch [2/5], Step [400/10], Loss: 0.0000\n",
      "Epoch [2/5], Step [500/10], Loss: 0.0000\n",
      "Epoch [2/5], Step [600/10], Loss: 0.0000\n",
      "Epoch [2/5], Step [700/10], Loss: 0.0000\n",
      "Epoch [2/5], Step [800/10], Loss: 0.0000\n",
      "Epoch [2/5], Step [900/10], Loss: 0.0001\n",
      "Epoch [2/5], Step [1000/10], Loss: 0.0000\n",
      "Epoch [3/5], Step [100/10], Loss: 0.0305\n",
      "Epoch [3/5], Step [200/10], Loss: 0.0000\n",
      "Epoch [3/5], Step [300/10], Loss: 0.0000\n",
      "Epoch [3/5], Step [400/10], Loss: 0.0000\n",
      "Epoch [3/5], Step [500/10], Loss: 0.0000\n",
      "Epoch [3/5], Step [600/10], Loss: 0.0000\n",
      "Epoch [3/5], Step [700/10], Loss: 0.0000\n",
      "Epoch [3/5], Step [800/10], Loss: 0.0000\n",
      "Epoch [3/5], Step [900/10], Loss: 0.0000\n",
      "Epoch [3/5], Step [1000/10], Loss: 0.0000\n",
      "Epoch [4/5], Step [100/10], Loss: 0.0067\n",
      "Epoch [4/5], Step [200/10], Loss: 0.0000\n",
      "Epoch [4/5], Step [300/10], Loss: 0.0000\n",
      "Epoch [4/5], Step [400/10], Loss: 0.0000\n",
      "Epoch [4/5], Step [500/10], Loss: 0.0000\n",
      "Epoch [4/5], Step [600/10], Loss: 0.0000\n",
      "Epoch [4/5], Step [700/10], Loss: 0.0000\n",
      "Epoch [4/5], Step [800/10], Loss: 0.0000\n",
      "Epoch [4/5], Step [900/10], Loss: 0.0000\n",
      "Epoch [4/5], Step [1000/10], Loss: 0.0000\n",
      "Epoch [5/5], Step [100/10], Loss: 0.0018\n",
      "Epoch [5/5], Step [200/10], Loss: 0.0000\n",
      "Epoch [5/5], Step [300/10], Loss: 0.0000\n",
      "Epoch [5/5], Step [400/10], Loss: 0.0000\n",
      "Epoch [5/5], Step [500/10], Loss: 0.0000\n",
      "Epoch [5/5], Step [600/10], Loss: 0.0000\n",
      "Epoch [5/5], Step [700/10], Loss: 0.0000\n",
      "Epoch [5/5], Step [800/10], Loss: 0.0000\n",
      "Epoch [5/5], Step [900/10], Loss: 0.0000\n",
      "Epoch [5/5], Step [1000/10], Loss: 0.0000\n",
      "1021\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in enumerate(train_loader):   # Load a batch of images with its (index, data, class)\n",
    "        data = Variable(data)       # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        outputs = net(data)                             # Forward pass: compute the output class given a image\n",
    "        labels = labels.long()\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimizer.step()                                  # Optimizer: update the weights of hidden nodes\n",
    "        \n",
    "        if (i+1) % 100 == 0:                              # Logging\n",
    "#             print(\"SMH\")\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data.item()))\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10K test images: 98 %\n",
      "correct: 1669\n",
      "total: 1703\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data, labels in test_loader:\n",
    "    data = Variable(data)\n",
    "    outputs = net(data)\n",
    "    _, predicted = torch.max(outputs.data, 1)  # Choose the best class from the output: The class with the best score\n",
    "    total += labels.size(0)                    # Increment the total count\n",
    "    labels = labels.long()\n",
    "    correct += (predicted == torch.max(labels, 1)[1]).sum()     # Increment the correct count\n",
    "    \n",
    "print('Accuracy of the network on the 10K test images: %d %%' % (100 * correct / total))\n",
    "print(\"correct:\", correct.item())\n",
    "print(\"total:\", total)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eulerspython",
   "language": "python",
   "name": "eulerspython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
