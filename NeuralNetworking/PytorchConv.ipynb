{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## from https://github.com/yunjey/pytorch-tutorial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as dsets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch.utils.data as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_size = 12       # The image size = 28 x 28 = 784\n",
    "# hidden_size = 500      # The number of nodes at the hidden layer\n",
    "# num_classes = 2       # The number of output classes. In this case, from 0 to 9\n",
    "# num_epochs = 5         # The number of times entire dataset is trained\n",
    "# batch_size = 100       # The size of input data took for one iteration\n",
    "# learning_rate = 0.000001  # The speed of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1703 rows of qcd data\n",
      "4605 rows of dihiggs data\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Import Dataset\n",
    "qcd_raw = pd.read_csv('../HiggsReconstruction/EventPlotting/qcd_outputDataForLearning.csv')\n",
    "hh_raw = pd.read_csv('../HiggsReconstruction/EventPlotting/dihiggs_outputDataForLearning.csv')\n",
    "\n",
    "qcd_raw.head()\n",
    "print(len(qcd_raw), \"rows of qcd data\")\n",
    "hh_raw.head()\n",
    "print(len(hh_raw), \"rows of dihiggs data\")\n",
    "\n",
    "# Make higgs and qcd sets from raw data\n",
    "# hh_all = hh_raw[['h1_mass', 'h2_mass', 'deltaR(h1 jets)', 'deltaR(h2 jets)', 'jet1_pz', 'jet2_pz', 'jet3_pz', 'jet4_pz', 'jet1_energy', 'jet2_energy', 'jet3_energy', 'jet4_energy']]\n",
    "# qcd = qcd_raw[['h1_mass', 'h2_mass', 'deltaR(h1 jets)', 'deltaR(h2 jets)', 'jet1_pz', 'jet2_pz', 'jet3_pz', 'jet4_pz', 'jet1_energy', 'jet2_energy', 'jet3_energy', 'jet4_energy']]\n",
    "hh_all = hh_raw[['deltaR(h1 jets)', 'deltaR(h2 jets)']]\n",
    "qcd = qcd_raw[['deltaR(h1 jets)', 'deltaR(h2 jets)']]\n",
    "n_factors = np.shape(hh_all)[1]\n",
    "print(n_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (4605) into shape (1703)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-83852969f523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# qcd[:, 0] = np.random.rand(np.shape(qcd)[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# qcd[:, 1] = np.random.rand(np.shape(qcd)[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0mqcd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhh_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0mqcd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhh_all\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (4605) into shape (1703)"
     ]
    }
   ],
   "source": [
    "hh_all = np.array(hh_all)\n",
    "qcd = np.array(qcd)\n",
    "\n",
    "# add labels to di-higgs\n",
    "hh_all=hh_all[:,:n_factors]\n",
    "# # print(hh[0:3])\n",
    "hh_labels= np.zeros((len(hh_all),1))\n",
    "hh_labels = hh_labels+1\n",
    "#a = hh[:len(hh)]\n",
    "# print(a.shape)\n",
    "hh_all[:,0] = np.random.rand(np.shape(hh_all)[0])\n",
    "hh_all = np.append(hh_all, hh_labels, axis=1)\n",
    "hh_all = np.append(hh_all, 1-hh_labels, axis=1)## hh qcd labels \n",
    "\n",
    "\n",
    "# print(hh.shape)\n",
    "# print(hh[0:3])\n",
    "\n",
    "# add labels to qcd\n",
    "qcd=qcd[:,:n_factors]\n",
    "# print(hh[0:3])\n",
    "qcd_labels= np.zeros((len(qcd),1))\n",
    "#a = hh[:len(hh)]\n",
    "# print(a.shape)\n",
    "# qcd hh labels \n",
    "qcd[:, 0] = -1 * np.random.rand(np.shape(qcd)[0])\n",
    "qcd = np.append(qcd, qcd_labels, axis=1) \n",
    "qcd = np.append(qcd, 1-qcd_labels, axis=1)# qcd qcd labels\n",
    "\n",
    "\n",
    "# use this for dummy variables\n",
    "hh_all[:,0] = np.random.rand(np.shape(hh_all)[0])\n",
    "hh_all[:,1] = np.random.rand(np.shape(hh_all)[0])\n",
    "# qcd[:, 0] = -1 * np.random.rand(np.shape(qcd)[0])\n",
    "# qcd[:, 1] = -1 * np.random.rand(np.shape(qcd)[0])\n",
    "# qcd[:, 0] = np.random.rand(np.shape(qcd)[0])\n",
    "# qcd[:, 1] = np.random.rand(np.shape(qcd)[0])\n",
    "qcd[:, 0] = hh_all[:,0]\n",
    "qcd[:, 1] = hh_all[:,1]\n",
    "\n",
    "\n",
    "# select a quarter of hh events so that the set is half and half\n",
    "# we shuffle the list first to take a random 1/4. this means we have a different dataset every time\n",
    "# np.random.seed(0)\n",
    "# np.random.shuffle(hh_all) \n",
    "hh = hh_all[0:len(qcd)]\n",
    "# print(hh[:4])\n",
    "# print(qcd[:4])\n",
    "\n",
    "all_data = np.append(hh,qcd, axis=0) \n",
    "all_data[:n_factors,:]\n",
    "\n",
    "np.random.seed(0)\n",
    "for i in range (4): # shuffle 4 times\n",
    "    np.random.shuffle(all_data) \n",
    "print(all_data[:4])\n",
    "all_labels = all_data[:,n_factors:]\n",
    "# for testing model resilience\n",
    "# for i in range(2):\n",
    "#     np.random.shuffle(all_labels)\n",
    "all_data = all_data[:,:n_factors]\n",
    "# print(all_data[:4])\n",
    "print(all_labels[:4])\n",
    "# print(test_data)\n",
    "# print(len(all_data))\n",
    "# print(all_labels)\n",
    "\n",
    "input_size = n_factors       # The image size = 28 x 28 = 784\n",
    "hidden_size = 500      # The number of nodes at the hidden layer\n",
    "num_classes = all_labels.shape[1]       # The number of output classes. In this case, from 0 to 9\n",
    "num_epochs = 5         # The number of times entire dataset is trained\n",
    "batch_size = 100       # The size of input data took for one iteration\n",
    "learning_rate = 0.001  # The speed of convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data by dividing it by the max value of each\n",
    "for i in range(np.shape(all_data)[1]):\n",
    "    all_data[:,i] = np.true_divide(all_data[:,i], np.max(all_data[:,i]))\n",
    "print(all_data[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_labels, test_labels = train_test_split(all_data, all_labels, train_size=0.3, test_size=0.5, random_state=42)\n",
    "\n",
    "train_data = torch.stack([torch.Tensor(i) for i in train_data])\n",
    "train_labels = torch.stack([torch.Tensor(i) for i in train_labels])\n",
    "test_data = torch.stack([torch.Tensor(i) for i in test_data])\n",
    "test_labels = torch.stack([torch.Tensor(i) for i in test_labels])\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = utils.TensorDataset(train_data, train_labels)\n",
    "\n",
    "test_dataset = utils.TensorDataset(test_data, test_labels)\n",
    "\n",
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = utils.DataLoader(train_dataset)\n",
    "\n",
    "test_loader = utils.DataLoader(test_dataset)\n",
    "print(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(enumerate(train_loader))\n",
    "for i, e in enumerate(train_loader):\n",
    "    print(i, e)\n",
    "    if(i>4):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(Net, self).__init__()                    # Inherited from the parent class nn.Module\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # 1st Full-Connected Layer: 784 (input data) -> 500 (hidden node)\n",
    "        self.relu = nn.ReLU()                          # Non-Linear ReLU Layer: max(0,x)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes) # 2nd Full-Connected Layer: 500 (hidden node) -> 10 (output class)\n",
    "    \n",
    "    def forward(self, x):                              # Forward pass: stacking each layer together\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net(input_size, hidden_size, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net.cuda()    # You can comment out this line to disable GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (data, labels) in enumerate(train_loader):   # Load a batch of images with its (index, data, class)\n",
    "        data = Variable(data)       # Convert torch tensor to Variable: change image from a vector of size 784 to a matrix of 28 x 28\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        optimizer.zero_grad()                             # Intialize the hidden weight to all zeros\n",
    "        outputs = net(data)                             # Forward pass: compute the output class given a image\n",
    "        labels = labels.long()\n",
    "        loss = criterion(outputs, torch.max(labels, 1)[1])                 # Compute the loss: difference between the output class and the pre-given label\n",
    "        loss.backward()                                   # Backward pass: compute the weight\n",
    "        optimizer.step()                                  # Optimizer: update the weights of hidden nodes\n",
    "        \n",
    "        if (i+1) % 100 == 0:                              # Logging\n",
    "#             print(\"SMH\")\n",
    "            print('Epoch [%d/%d], Step [%d/%d], Loss: %.4f'\n",
    "                 %(epoch+1, num_epochs, i+1, len(train_dataset)//batch_size, loss.data.item()))\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for data, labels in test_loader:\n",
    "    data = Variable(data)\n",
    "    outputs = net(data)\n",
    "    _, predicted = torch.max(outputs.data, 1)  # Choose the best class from the output: The class with the best score\n",
    "    total += labels.size(0)                    # Increment the total count\n",
    "    labels = labels.long()\n",
    "    correct += (predicted == torch.max(labels, 1)[1]).sum()     # Increment the correct count\n",
    "    \n",
    "print('Accuracy of the network on the 10K test images: %d %%' % (100 * correct / total))\n",
    "print(correct)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eulerspython",
   "language": "python",
   "name": "eulerspython"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
